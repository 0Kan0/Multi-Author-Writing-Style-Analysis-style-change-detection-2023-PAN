{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by:\n",
    "* Alberto Cano Turnes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi-Author Writing Style Analysis (style change detection) 2023**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Importing Python Libraries and preparing the environment**\n",
    "\n",
    "At this step we will be importing the libraries and modules needed to run our script. Libraries are:\n",
    "* Transformers\n",
    "* Transformers tokenizers for RoBERTa, BERT and DistilBERT\n",
    "* Pytorch\n",
    "* Pytorch Utils for Dataset and Dataloader\n",
    "* Numpy\n",
    "* Pandas\n",
    "* os\n",
    "* re\n",
    "* json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sonic\\Desktop\\a\\env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import RobertaTokenizer, BertTokenizer, DistilBertTokenizer\n",
    "import torch\n",
    "from torch import cuda\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting constant variables\n",
    "\n",
    "We set some constant variables that will be used for the Dataloader and for pre-processing the data. We also set the tokenizer and cuda as the device (in case we have a GPU, otherwise, the CPU will be used)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining some key variables that will be used later on in the training\n",
    "MAX_LEN = 128 # @param {type:\"integer\"}\n",
    "TRAIN_BATCH_SIZE = 32 # @param {type:\"integer\"}\n",
    "VALID_BATCH_SIZE = 16 # @param {type:\"integer\"}\n",
    "EPOCHS = 8 # @param {type:\"integer\"}\n",
    "LEARNING_RATE = 1e-5 # @param {type:\"number\"}\n",
    "\n",
    "# Defining paths to the data\n",
    "TRAIN_EASY_PATH=\"./data/dataset_easy/train/\" # @param {type:\"string\"}\n",
    "TRAIN_MEDIUM_PATH=\"./data/dataset_medium/train/\" # @param {type:\"string\"}\n",
    "TRAIN_HARD_PATH=\"./data/dataset_hard/train/\" # @param {type:\"string\"}\n",
    "VALIDATION_EASY_PATH=\"./data/dataset_easy/validation/\" # @param {type:\"string\"}\n",
    "VALIDATION_MEDIUM_PATH=\"./data/dataset_medium/validation/\" # @param {type:\"string\"}\n",
    "VALIDATION_HARD_PATH=\"./data/dataset_hard/validation/\" # @param {type:\"string\"}\n",
    "\n",
    "# Setting the tokenizer to be used\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "# Setting up the device for GPU usage\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Importing and Pre-Processing the domain data**\n",
    "\n",
    "In this next part, we will do the necessary steps to load the dataset and pre-process it in order to carry out the chosen task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to import and pre-process the data\n",
    "\n",
    "In this cell below, we declare some functions that we will be used to import and pre-process the data provided. The functions as follows:\n",
    "\n",
    "* `txt_to_array(txt_path)`: This function receives the path of the txt files with the paragraphs and return its content as a list of paragraphs.\n",
    "\n",
    "* `get_json_labels(json_path)`: This function receives the path of the json files with the labels and return its content as a dictionary.\n",
    "\n",
    "* `load_files(data_path)`: This function loads the txt and json files from a given path.\n",
    "\n",
    "* `concatenate_paragraphs(df)`: This function concatenate adjacent paragraphs from a dataframe into a new dataframe.\n",
    "\n",
    "* `create_csv(train_path, validation_path)`: This function create the final CSV file from text and label files that will be used to train and test the models.\n",
    "\n",
    "Since we are asked to check if 2 consecutive paragraphs were written by the same author, we think that splitting the data into consecutive paragraphs and concetenate is a good idea,since this will allow us to transform the task into a binary classification problem where 0 means the paragraphs were written by the same author and 1 means they were written by different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read a text file and return its content as a list of paragraphs\n",
    "def txt_to_array(txt_path):\n",
    "    with open(txt_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    paragraph_list = content.split('\\n')\n",
    "\n",
    "    return paragraph_list\n",
    "\n",
    "# Function to read a JSON file and return its content as a dictionary\n",
    "def get_json_labels(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        \n",
    "    return json.loads(content)\n",
    "\n",
    "# Function to load text and label files from a given directory\n",
    "def load_files(data_path):\n",
    "    paragraph4files = {}\n",
    "    labels4files = {}\n",
    "    for file in os.listdir(data_path):\n",
    "        if file.startswith('problem-') and file.endswith('.txt'):\n",
    "            matches = re.search(r'problem-(\\d+).txt', file)\n",
    "\n",
    "            if matches:\n",
    "                file_number = int(matches.group(1))\n",
    "                file_path = os.path.join(data_path, file)\n",
    "                labels_path = os.path.join(data_path,f\"truth-problem-{file_number}.json\")\n",
    "\n",
    "                paragraph_list = txt_to_array(file_path)\n",
    "                paragraph4files[file_number-1] = paragraph_list\n",
    "                labels4files[file_number-1] = get_json_labels(labels_path)\n",
    "\n",
    "    return paragraph4files, labels4files\n",
    "\n",
    "# Function to concatenate adjacent paragraphs in a dataframe\n",
    "def concatenate_paragraphs(df):\n",
    "    new_texts = []\n",
    "    new_changes = []\n",
    "    for i in range(len(df) - 1):\n",
    "        combined_texts = []\n",
    "        changes = []\n",
    "        for j in range(len(df['texts'][i]) - 1):\n",
    "            combined_text = df['texts'][i][j] + '[CLS]' + df['texts'][i][j + 1] # [CLS] token is used to separate paragraphs so that the model can learn to distinguish between them\n",
    "            change = [df['changes'][i][j]]\n",
    "            combined_texts.append(combined_text)\n",
    "            changes.append(change)\n",
    "\n",
    "        new_texts.extend(combined_texts)\n",
    "        new_changes.extend(changes)\n",
    "\n",
    "    return new_texts, new_changes\n",
    "\n",
    "# Function to create a CSV file from text and label files\n",
    "def create_csv(train_path, validation_path):\n",
    "    train_paragraphs, train_labels = load_files(train_path)\n",
    "    validation_paragraphs, validation_labels = load_files(validation_path)\n",
    "\n",
    "    train_texts = list(train_paragraphs.values())\n",
    "    train_changes = [v['changes'] for k, v in train_labels.items()]\n",
    "\n",
    "    validation_texts = list(validation_paragraphs.values())\n",
    "    validation_changes = [v['changes'] for k, v in validation_labels.items()]\n",
    "\n",
    "    df_train = pd.DataFrame({'texts': train_texts, 'changes': train_changes})\n",
    "    df_validation = pd.DataFrame({'texts': validation_texts, 'changes': validation_changes})\n",
    "\n",
    "    concatenated_train_paragraphs, paragraph_train_change = concatenate_paragraphs(df_train)\n",
    "    concatenated_validation_paragraphs, paragraph_validation_change = concatenate_paragraphs(df_validation)\n",
    "\n",
    "    df_train = pd.DataFrame({'texts': concatenated_train_paragraphs, 'changes': paragraph_train_change})\n",
    "    df_validation = pd.DataFrame({'texts': concatenated_validation_paragraphs, 'changes': paragraph_validation_change})\n",
    "    \n",
    "    df_train.to_csv('data/dataset_csv/' + train_path.split('/')[2].lower() + '_train.csv', index=False)\n",
    "    df_validation.to_csv('data/dataset_csv/' + validation_path.split('/')[2].lower()+ '_validation.csv', index=False)\n",
    "\n",
    "    return df_train, df_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataframes for each difficulty\n",
    "\n",
    "We create 6 datasets from the data provided, 3 for training and 3 for validation. This 6 datasets are based on the difficulty of the data provided:\n",
    "* Easy: We create `easy_df_train` and `easy_validation_df` csv.\n",
    "* Medium: We create `medium_df_train` and `medium_df_validation` csv.\n",
    "* Hard: We create `hard_df_train` and `hard_df_validation` csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy train dataset:\n",
      "                                               texts changes\n",
      "0  I'm not arguing with you here, I'm simply tryi...     [1]\n",
      "1  He's at my place half the time and his fiance(...     [0]\n",
      "2  Biden has the power to write an executive orde...     [1]\n",
      "3  r/politics is currently accepting new moderato...     [1]\n",
      "4  The inflation reduction act increases inflatio...     [0]\n",
      "\n",
      "\n",
      "Easy validation dataset:\n",
      "                                               texts changes\n",
      "0  I think the premise of your question is off to...     [1]\n",
      "1  You bought this in good faith and without clai...     [1]\n",
      "2  Yes. That section of the code says what it say...     [1]\n",
      "3  She doesnt “savage” anyone, no one is “fuming”...     [1]\n",
      "4  I left Ohio in 1998. Never looked back. I've l...     [1]\n"
     ]
    }
   ],
   "source": [
    "# Create easy train dataset and validation dataset from data folder and save them as csv files\n",
    "easy_df_train, easy_df_validation = create_csv(TRAIN_EASY_PATH, VALIDATION_EASY_PATH)\n",
    "\n",
    "print(\"Easy train dataset:\")\n",
    "print(easy_df_train.head())\n",
    "\n",
    "print(\"\\n\\nEasy validation dataset:\")\n",
    "print(easy_df_validation.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium train dataset:\n",
      "                                               texts changes\n",
      "0  Nevada has some very generous laws for squatte...     [0]\n",
      "1  The timing of their entry is very suspect and ...     [0]\n",
      "2  There may be sensitive information related to ...     [1]\n",
      "3  Thank you for the info about the eviction firm...     [1]\n",
      "4  Also the equipment they're largely relying on ...     [1]\n",
      "\n",
      "\n",
      "Medium validation dataset:\n",
      "                                               texts changes\n",
      "0  I asked them about something of that nature an...     [1]\n",
      "1  You probably need to secure the waiver from th...     [0]\n",
      "2  I don’t know which school you’re at but at lea...     [1]\n",
      "3  I have been involved in appeals for classes fo...     [1]\n",
      "4  Did you start petitioning with the disability ...     [1]\n"
     ]
    }
   ],
   "source": [
    "# Create medium train dataset and validation dataset from data folder and save them as csv files\n",
    "medium_df_train, medium_df_validation = create_csv(TRAIN_MEDIUM_PATH, VALIDATION_MEDIUM_PATH)\n",
    "\n",
    "print(\"Medium train dataset:\")\n",
    "print(medium_df_train.head())\n",
    "\n",
    "print(\"\\n\\nMedium validation dataset:\")\n",
    "print(medium_df_validation.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard train dataset:\n",
      "                                               texts changes\n",
      "0  Great post. I'd add that another aspect of com...     [1]\n",
      "1  A major philosophical challenger to this versi...     [0]\n",
      "2  One additional irony that I would add is that ...     [0]\n",
      "3  This leaves us with the third \"national projec...     [0]\n",
      "4  Im also well aware of the numerous atrocities ...     [0]\n",
      "\n",
      "\n",
      "Hard validation dataset:\n",
      "                                               texts changes\n",
      "0  Everybody knows t was the leader of the senate...     [0]\n",
      "1  Naw it came off in a negative way. Like making...     [1]\n",
      "2  And I think there may also be other tactics, l...     [1]\n",
      "3  Look… it’s partly true what you’re saying but,...     [1]\n",
      "4  Maybe not lawmakers, but if a resident of Oreg...     [1]\n"
     ]
    }
   ],
   "source": [
    "# Create hard train dataset and validation dataset from data folder and save them as csv files\n",
    "hard_df_train, hard_df_validation = create_csv(TRAIN_HARD_PATH, VALIDATION_HARD_PATH)\n",
    "\n",
    "print(\"Hard train dataset:\")\n",
    "print(hard_df_train.head())\n",
    "\n",
    "print(\"\\n\\nHard validation dataset:\")\n",
    "print(hard_df_validation.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we see the changes of each dataframe, we noticed that the `easy_df_train` is unbalanced (theres much more 1 than 0). Because of this, we will create a sampler to balance this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "changes\n",
       "[1]    11345\n",
       "[0]     1557\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "easy_df_train[\"changes\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "changes\n",
       "[0]    15000\n",
       "[1]    13211\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medium_df_train[\"changes\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "changes\n",
       "[0]    10090\n",
       "[1]     9018\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hard_df_train[\"changes\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom dataset class\n",
    "\n",
    "The class created below has been designed to preprocess text from the dataframes, encode it using a tokenizer (in our case we are using the RoBERTa tokenizer), and prepare the data to use it in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParagraphDataset(Dataset):\n",
    "    # Initialize the dataset\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        \n",
    "        self.tokenizer = tokenizer  # Store the provided tokenizer\n",
    "        self.data = dataframe  # Store the dataframe\n",
    "        self.texts = dataframe.texts  # Extract the 'texts' column from the dataframe\n",
    "        self.targets = self.data.changes  # Extract the 'changes' column from the dataframe\n",
    "        self.max_len = max_len  # Store the maximum length\n",
    "\n",
    "    # Return the length of the dataset based on the number of items in 'texts'\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    # Get a specific item from the dataset based on the index 'idx'\n",
    "    def __getitem__(self, idx):\n",
    "        texts = self.texts[idx]  # Fetch the text corresponding to the index\n",
    "        \n",
    "        # Tokenize the text using the provided tokenizer\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            texts,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "\n",
    "        ids = encoding['input_ids']  # Extract token IDs\n",
    "        mask = encoding['attention_mask']  # Extract attention masks\n",
    "\n",
    "        # Return a dictionary containing token IDs, attention masks, target values, and the original text\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.targets[idx]),\n",
    "            'texts': texts\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader functions\n",
    "\n",
    "In this cell below, we declare some functions that we will be used to load the data into the neural network in a defined manner. The functions are as follows:\n",
    "\n",
    "* `create_sampler(df_train)`: This function receives the dataframe used for training and creates a weighted random sampler based on the class distribution.\n",
    "\n",
    "* `create_dataloader(difficulty, df_train, df_validation, tokenizer, sampler)`: This function receives both train and validation dataframes, the tokenizer and the sampler and creates the training and validation dataloaders that loads the data into the neural network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a weighted random sampler based on the class distribution in the training dataset\n",
    "def create_sampler(df_train):\n",
    "    \n",
    "    if isinstance(df_train['changes'].iloc[0], list):\n",
    "        df_train['changes'] = df_train['changes'].apply(lambda x: x[0])\n",
    "\n",
    "    class_counts = df_train['changes'].value_counts()\n",
    "    num_samples = len(df_train)\n",
    "\n",
    "    weights = 1. / class_counts[df_train['changes']]\n",
    "    weights = weights.tolist()\n",
    "    \n",
    "    sampler = WeightedRandomSampler(weights, num_samples)\n",
    "\n",
    "    return sampler\n",
    "\n",
    "# Function to create data loaders for training and testing datasets\n",
    "def create_dataloader(df_train, df_validation, tokenizer, sampler):\n",
    "\n",
    "    train_dataset = df_train.sample(frac=1, random_state=200)\n",
    "    test_dataset = df_validation.sample(frac=1)\n",
    "    train_dataset = train_dataset.reset_index(drop=True)\n",
    "    test_dataset = test_dataset.reset_index(drop=True)\n",
    "    \n",
    "    print(\"FULL Dataset: {}\".format((df_train.shape[0] + df_validation.shape[0], df_train.shape[1])))\n",
    "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "    print(\"TEST Dataset: {}\\n\".format(test_dataset.shape))\n",
    "\n",
    "    training_set = ParagraphDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "    testing_set = ParagraphDataset(test_dataset, tokenizer, MAX_LEN)\n",
    "\n",
    "    train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
    "                    'sampler': sampler,\n",
    "                    'num_workers': 0,\n",
    "                    }\n",
    "\n",
    "    test_params = {'batch_size': VALID_BATCH_SIZE,\n",
    "                    'num_workers': 0,\n",
    "                    }\n",
    "\n",
    "    training_loader = DataLoader(training_set, **train_params)\n",
    "    testing_loader = DataLoader(testing_set, **test_params)\n",
    "\n",
    "    return training_loader, testing_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the dataloaders for each difficulty\n",
    "\n",
    "We create 6 dataloaders from the dataframes provided, 3 for training and 3 for validation. This 6 dataloaders are based on the difficulty of the data provided:\n",
    "* Easy: We create `easy_training_loader` and `easy_testing_loader`.\n",
    "* Medium: We create `medium_training_loader` and `medium_testing_loader`.\n",
    "* Hard: We create `hard_training_loader` and `hard_testing_loader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy:\n",
      "FULL Dataset: (15729, 2)\n",
      "TRAIN Dataset: (12902, 2)\n",
      "TEST Dataset: (2827, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a weighted random sampler for the easy dataset\n",
    "easy_sampler = create_sampler(easy_df_train)\n",
    "\n",
    "# Create dataloaders for the easy dataset\n",
    "print(\"Easy:\")\n",
    "easy_training_loader, easy_testing_loader = create_dataloader(easy_df_train, easy_df_validation, tokenizer, easy_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium:\n",
      "FULL Dataset: (35242, 2)\n",
      "TRAIN Dataset: (28211, 2)\n",
      "TEST Dataset: (7031, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a weighted random sampler for the medium dataset\n",
    "medium_sampler = create_sampler(medium_df_train)\n",
    "\n",
    "# Create dataloaders for the medium dataset\n",
    "print(\"Medium:\")\n",
    "medium_training_loader, medium_testing_loader = create_dataloader(medium_df_train, medium_df_validation, tokenizer, medium_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard:\n",
      "FULL Dataset: (23218, 2)\n",
      "TRAIN Dataset: (19108, 2)\n",
      "TEST Dataset: (4110, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a weighted random sampler for the hard dataset\n",
    "hard_sampler = create_sampler(hard_df_train)\n",
    "\n",
    "# Create dataloaders for the hard dataset\n",
    "print(\"Hard:\")\n",
    "hard_training_loader, hard_testing_loader = create_dataloader(hard_df_train, hard_df_validation, tokenizer, hard_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Creating the Neural Network for Fine Tuning**\n",
    "\n",
    "The `RoBERTaClass` is a custom neural network model that utilizes the RoBERTa transformer model for natural language processing tasks. It is designed to perform classification tasks by taking input text sequences and predicting a binary output.\n",
    "\n",
    "The class consists of three main layers:\n",
    "1. `self.l1`: This layer represents the RoBERTa model, which is a pre-trained transformer model for language understanding. It is loaded from the 'roberta-base' pre-trained model using the `transformers` library.\n",
    "2. `self.l2`: This layer is a dropout layer that helps prevent overfitting by randomly dropping out a fraction of the input units during training.\n",
    "3. `self.l3`: This layer is a linear layer that maps the output of the RoBERTa model to a single output value. It performs the final classification based on the learned representations from the previous layers.\n",
    "\n",
    "The `forward()` method defines the forward pass of the model. It takes input `ids` (tokenized input sequence) and `mask` (attention mask) as input and passes them through the layers in the defined order. The output of the model is the predicted output value.\n",
    "\n",
    "In summary, the `RoBERTaClass` encapsulates the RoBERTa model and provides a convenient interface for performing classification tasks on text data.\n",
    "\n",
    "We choose RoBERTa over other transofrmer models due to the good results it has given compared to other models like BERT or DistilBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RoBERTaClass\n",
    "class RoBERTaClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RoBERTaClass, self).__init__()\n",
    "        self.dropout = 0.5\n",
    "        self.hidden_embd = 768\n",
    "        self.output_layer = 1\n",
    "        # Declare the layers here\n",
    "        self.l1 = transformers.RobertaModel.from_pretrained('roberta-base')\n",
    "        self.l2 = torch.nn.Dropout(self.dropout)\n",
    "        self.l3 = torch.nn.Linear(self.hidden_embd, self.output_layer)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        # Use the transformer, then the dropout and the linear in that order.\n",
    "        _, output_1 = self.l1(ids, attention_mask = mask, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the loss function\n",
    "\n",
    "The loss function used will be a combination of Binary Cross Entropy which is implemented as [BCELogits Loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "def loss_fn(outputs, targets):\n",
    "    return torch.nn.BCEWithLogitsLoss()(outputs.view(-1), targets.view(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the models for each difficulty\n",
    "\n",
    "We create 3 RoBERTa models for each difficulty alongside their optimizers:\n",
    "* Easy: We create `easy_model` and `easy_optimizer`.\n",
    "* Medium: We create `medium_model` and `medium_optimizer`.\n",
    "* Hard: We create `hard_model` and `hard_optimizer`.\n",
    "\n",
    "The optimizers are used to update the weights of the neural network to improve its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the RoBERTaClass for easy dataset\n",
    "easy_model = RoBERTaClass().to(device)\n",
    "\n",
    "# Define the optimizers for easy dataset\n",
    "easy_optimizer = torch.optim.Adam(params = easy_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the RoBERTaClass for medium dataset\n",
    "medium_model = RoBERTaClass().to(device)\n",
    "\n",
    "# Define the optimizers for medium dataset\n",
    "medium_optimizer = torch.optim.Adam(params = medium_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the RoBERTaClass for hard dataset\n",
    "hard_model = RoBERTaClass().to(device)\n",
    "\n",
    "# Define the optimizers for hard dataset\n",
    "hard_optimizer = torch.optim.Adam(params = hard_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training setup\n",
    "\n",
    "The `train` function is a training loop that is used to train a neural network model on a given dataset. It takes as input the model, training data loader, optimizer, and the current epoch number. \n",
    "\n",
    "Inside the function, it sets the model to training mode using `model.train()`. Then, it iterates over the batches of data in the training loader. For each batch, it performs the following steps:\n",
    "\n",
    "1. Moves the input data (`ids`, `mask`, and `targets`) to the device (e.g., GPU) for computation.\n",
    "2. Passes the input data through the model to obtain the predicted outputs.\n",
    "3. Zeros out the gradients of the optimizer.\n",
    "4. Computes the loss between the predicted outputs and the target values using the `loss_fn` function.\n",
    "5. Prints the loss value every 10 iterations.\n",
    "6. Performs backpropagation to compute the gradients of the model parameters with respect to the loss.\n",
    "7. Updates the model parameters using the optimizer's update rule.\n",
    "\n",
    "The purpose of this function is to train the model by optimizing its parameters based on the provided training data and the specified loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training function\n",
    "\n",
    "def train(difficulty, model, training_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for _,data in enumerate(training_loader, 0):\n",
    "       \n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.float)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        if _%10==0:\n",
    "            print(f'{difficulty} dataset, Epoch: {epoch}, Loss:  {loss.item()}')\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the training\n",
    "\n",
    "The cells below initialize the training for each difficulty dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\sonic\\Desktop\\a\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Easy dataset, Epoch: 0, Loss:  0.7009953260421753\n",
      "Easy dataset, Epoch: 0, Loss:  0.5989694595336914\n",
      "Easy dataset, Epoch: 0, Loss:  0.44915711879730225\n",
      "Easy dataset, Epoch: 0, Loss:  0.302805095911026\n",
      "Easy dataset, Epoch: 0, Loss:  0.4931529760360718\n",
      "Easy dataset, Epoch: 0, Loss:  0.3349299132823944\n",
      "Easy dataset, Epoch: 0, Loss:  0.309378445148468\n",
      "Easy dataset, Epoch: 0, Loss:  0.43250390887260437\n",
      "Easy dataset, Epoch: 0, Loss:  0.3323203921318054\n",
      "Easy dataset, Epoch: 0, Loss:  0.3676379323005676\n",
      "Easy dataset, Epoch: 0, Loss:  0.2547387480735779\n",
      "Easy dataset, Epoch: 0, Loss:  0.26252469420433044\n",
      "Easy dataset, Epoch: 0, Loss:  0.27132314443588257\n",
      "Easy dataset, Epoch: 0, Loss:  0.2526576817035675\n",
      "Easy dataset, Epoch: 0, Loss:  0.1399601250886917\n",
      "Easy dataset, Epoch: 0, Loss:  0.09294439852237701\n",
      "Easy dataset, Epoch: 0, Loss:  0.24725991487503052\n",
      "Easy dataset, Epoch: 0, Loss:  0.2784508466720581\n",
      "Easy dataset, Epoch: 0, Loss:  0.15708550810813904\n",
      "Easy dataset, Epoch: 0, Loss:  0.22153688967227936\n",
      "Easy dataset, Epoch: 0, Loss:  0.24131862819194794\n",
      "Easy dataset, Epoch: 0, Loss:  0.21922272443771362\n",
      "Easy dataset, Epoch: 0, Loss:  0.1138085350394249\n",
      "Easy dataset, Epoch: 0, Loss:  0.2729402482509613\n",
      "Easy dataset, Epoch: 0, Loss:  0.0794939175248146\n",
      "Easy dataset, Epoch: 0, Loss:  0.20227760076522827\n",
      "Easy dataset, Epoch: 0, Loss:  0.2531164586544037\n",
      "Easy dataset, Epoch: 0, Loss:  0.09096568822860718\n",
      "Easy dataset, Epoch: 0, Loss:  0.11522676050662994\n",
      "Easy dataset, Epoch: 0, Loss:  0.10953742265701294\n",
      "Easy dataset, Epoch: 0, Loss:  0.19971808791160583\n",
      "Easy dataset, Epoch: 0, Loss:  0.17193461954593658\n",
      "Easy dataset, Epoch: 0, Loss:  0.054318543523550034\n",
      "Easy dataset, Epoch: 0, Loss:  0.051195479929447174\n",
      "Easy dataset, Epoch: 0, Loss:  0.10721047967672348\n",
      "Easy dataset, Epoch: 0, Loss:  0.22472168505191803\n",
      "Easy dataset, Epoch: 0, Loss:  0.16257745027542114\n",
      "Easy dataset, Epoch: 0, Loss:  0.04151954874396324\n",
      "Easy dataset, Epoch: 0, Loss:  0.04260553419589996\n",
      "Easy dataset, Epoch: 0, Loss:  0.022842835634946823\n",
      "Easy dataset, Epoch: 0, Loss:  0.0478195920586586\n",
      "Easy dataset, Epoch: 1, Loss:  0.01275717280805111\n",
      "Easy dataset, Epoch: 1, Loss:  0.049236755818128586\n",
      "Easy dataset, Epoch: 1, Loss:  0.017018049955368042\n",
      "Easy dataset, Epoch: 1, Loss:  0.09239092469215393\n",
      "Easy dataset, Epoch: 1, Loss:  0.07730662822723389\n",
      "Easy dataset, Epoch: 1, Loss:  0.028845228254795074\n",
      "Easy dataset, Epoch: 1, Loss:  0.05986605957150459\n",
      "Easy dataset, Epoch: 1, Loss:  0.01841631717979908\n",
      "Easy dataset, Epoch: 1, Loss:  0.08544440567493439\n",
      "Easy dataset, Epoch: 1, Loss:  0.2695023715496063\n",
      "Easy dataset, Epoch: 1, Loss:  0.02227056585252285\n",
      "Easy dataset, Epoch: 1, Loss:  0.11672549694776535\n",
      "Easy dataset, Epoch: 1, Loss:  0.037565309554338455\n",
      "Easy dataset, Epoch: 1, Loss:  0.11266680061817169\n",
      "Easy dataset, Epoch: 1, Loss:  0.04392709583044052\n",
      "Easy dataset, Epoch: 1, Loss:  0.05719669908285141\n",
      "Easy dataset, Epoch: 1, Loss:  0.029423300176858902\n",
      "Easy dataset, Epoch: 1, Loss:  0.22633899748325348\n",
      "Easy dataset, Epoch: 1, Loss:  0.2277400940656662\n",
      "Easy dataset, Epoch: 1, Loss:  0.01812133379280567\n",
      "Easy dataset, Epoch: 1, Loss:  0.2619868814945221\n",
      "Easy dataset, Epoch: 1, Loss:  0.22632881999015808\n",
      "Easy dataset, Epoch: 1, Loss:  0.08648832142353058\n",
      "Easy dataset, Epoch: 1, Loss:  0.16180720925331116\n",
      "Easy dataset, Epoch: 1, Loss:  0.08482396602630615\n",
      "Easy dataset, Epoch: 1, Loss:  0.030270908027887344\n",
      "Easy dataset, Epoch: 1, Loss:  0.10623519122600555\n",
      "Easy dataset, Epoch: 1, Loss:  0.08293873071670532\n",
      "Easy dataset, Epoch: 1, Loss:  0.08272645622491837\n",
      "Easy dataset, Epoch: 1, Loss:  0.07051320374011993\n",
      "Easy dataset, Epoch: 1, Loss:  0.00782198179513216\n",
      "Easy dataset, Epoch: 1, Loss:  0.023226985707879066\n",
      "Easy dataset, Epoch: 1, Loss:  0.007016710937023163\n",
      "Easy dataset, Epoch: 1, Loss:  0.017812630161643028\n",
      "Easy dataset, Epoch: 1, Loss:  0.0449463427066803\n",
      "Easy dataset, Epoch: 1, Loss:  0.014542155899107456\n",
      "Easy dataset, Epoch: 1, Loss:  0.01368449255824089\n",
      "Easy dataset, Epoch: 1, Loss:  0.01139676384627819\n",
      "Easy dataset, Epoch: 1, Loss:  0.1730717569589615\n",
      "Easy dataset, Epoch: 1, Loss:  0.08214263617992401\n",
      "Easy dataset, Epoch: 1, Loss:  0.0407569520175457\n",
      "Easy dataset, Epoch: 2, Loss:  0.011734480038285255\n",
      "Easy dataset, Epoch: 2, Loss:  0.09577545523643494\n",
      "Easy dataset, Epoch: 2, Loss:  0.08015803247690201\n",
      "Easy dataset, Epoch: 2, Loss:  0.024366173893213272\n",
      "Easy dataset, Epoch: 2, Loss:  0.06684967875480652\n",
      "Easy dataset, Epoch: 2, Loss:  0.043000392615795135\n",
      "Easy dataset, Epoch: 2, Loss:  0.0332588329911232\n",
      "Easy dataset, Epoch: 2, Loss:  0.20797199010849\n",
      "Easy dataset, Epoch: 2, Loss:  0.0276597011834383\n",
      "Easy dataset, Epoch: 2, Loss:  0.005933167412877083\n",
      "Easy dataset, Epoch: 2, Loss:  0.05001324042677879\n",
      "Easy dataset, Epoch: 2, Loss:  0.10349778085947037\n",
      "Easy dataset, Epoch: 2, Loss:  0.2093733549118042\n",
      "Easy dataset, Epoch: 2, Loss:  0.06809971481561661\n",
      "Easy dataset, Epoch: 2, Loss:  0.01979181542992592\n",
      "Easy dataset, Epoch: 2, Loss:  0.11295227706432343\n",
      "Easy dataset, Epoch: 2, Loss:  0.1693640798330307\n",
      "Easy dataset, Epoch: 2, Loss:  0.013066803105175495\n",
      "Easy dataset, Epoch: 2, Loss:  0.010146597400307655\n",
      "Easy dataset, Epoch: 2, Loss:  0.055719561874866486\n",
      "Easy dataset, Epoch: 2, Loss:  0.01946651190519333\n",
      "Easy dataset, Epoch: 2, Loss:  0.004806801211088896\n",
      "Easy dataset, Epoch: 2, Loss:  0.024498729035258293\n",
      "Easy dataset, Epoch: 2, Loss:  0.038712382316589355\n",
      "Easy dataset, Epoch: 2, Loss:  0.04493140056729317\n",
      "Easy dataset, Epoch: 2, Loss:  0.004851310979574919\n",
      "Easy dataset, Epoch: 2, Loss:  0.007040258031338453\n",
      "Easy dataset, Epoch: 2, Loss:  0.1159234419465065\n",
      "Easy dataset, Epoch: 2, Loss:  0.04342469573020935\n",
      "Easy dataset, Epoch: 2, Loss:  0.02245357073843479\n",
      "Easy dataset, Epoch: 2, Loss:  0.06219299137592316\n",
      "Easy dataset, Epoch: 2, Loss:  0.16352838277816772\n",
      "Easy dataset, Epoch: 2, Loss:  0.014885966666042805\n",
      "Easy dataset, Epoch: 2, Loss:  0.0035906247794628143\n",
      "Easy dataset, Epoch: 2, Loss:  0.15866029262542725\n",
      "Easy dataset, Epoch: 2, Loss:  0.07728059589862823\n",
      "Easy dataset, Epoch: 2, Loss:  0.04252294450998306\n",
      "Easy dataset, Epoch: 2, Loss:  0.024286016821861267\n",
      "Easy dataset, Epoch: 2, Loss:  0.11732619255781174\n",
      "Easy dataset, Epoch: 2, Loss:  0.02358950302004814\n",
      "Easy dataset, Epoch: 2, Loss:  0.006982846185564995\n",
      "Easy dataset, Epoch: 3, Loss:  0.009007887914776802\n",
      "Easy dataset, Epoch: 3, Loss:  0.005573147442191839\n",
      "Easy dataset, Epoch: 3, Loss:  0.2119588404893875\n",
      "Easy dataset, Epoch: 3, Loss:  0.12514746189117432\n",
      "Easy dataset, Epoch: 3, Loss:  0.004284149035811424\n",
      "Easy dataset, Epoch: 3, Loss:  0.013381507247686386\n",
      "Easy dataset, Epoch: 3, Loss:  0.017584629356861115\n",
      "Easy dataset, Epoch: 3, Loss:  0.005251293070614338\n",
      "Easy dataset, Epoch: 3, Loss:  0.002803962677717209\n",
      "Easy dataset, Epoch: 3, Loss:  0.0056855566799640656\n",
      "Easy dataset, Epoch: 3, Loss:  0.04892250895500183\n",
      "Easy dataset, Epoch: 3, Loss:  0.013455349951982498\n",
      "Easy dataset, Epoch: 3, Loss:  0.008564670570194721\n",
      "Easy dataset, Epoch: 3, Loss:  0.004118969198316336\n",
      "Easy dataset, Epoch: 3, Loss:  0.019001133739948273\n",
      "Easy dataset, Epoch: 3, Loss:  0.01785713993012905\n",
      "Easy dataset, Epoch: 3, Loss:  0.00854517798870802\n",
      "Easy dataset, Epoch: 3, Loss:  0.00225022342056036\n",
      "Easy dataset, Epoch: 3, Loss:  0.08849434554576874\n",
      "Easy dataset, Epoch: 3, Loss:  0.004652911797165871\n",
      "Easy dataset, Epoch: 3, Loss:  0.039655860513448715\n",
      "Easy dataset, Epoch: 3, Loss:  0.05079057812690735\n",
      "Easy dataset, Epoch: 3, Loss:  0.00510210357606411\n",
      "Easy dataset, Epoch: 3, Loss:  0.007670633960515261\n",
      "Easy dataset, Epoch: 3, Loss:  0.030241485685110092\n",
      "Easy dataset, Epoch: 3, Loss:  0.006049751304090023\n",
      "Easy dataset, Epoch: 3, Loss:  0.055001530796289444\n",
      "Easy dataset, Epoch: 3, Loss:  0.1041979044675827\n",
      "Easy dataset, Epoch: 3, Loss:  0.13621516525745392\n",
      "Easy dataset, Epoch: 3, Loss:  0.05959043279290199\n",
      "Easy dataset, Epoch: 3, Loss:  0.21142929792404175\n",
      "Easy dataset, Epoch: 3, Loss:  0.35048726201057434\n",
      "Easy dataset, Epoch: 3, Loss:  0.04927409440279007\n",
      "Easy dataset, Epoch: 3, Loss:  0.09548552334308624\n",
      "Easy dataset, Epoch: 3, Loss:  0.02120058238506317\n",
      "Easy dataset, Epoch: 3, Loss:  0.009765071794390678\n",
      "Easy dataset, Epoch: 3, Loss:  0.2807174026966095\n",
      "Easy dataset, Epoch: 3, Loss:  0.012532925233244896\n",
      "Easy dataset, Epoch: 3, Loss:  0.016353348270058632\n",
      "Easy dataset, Epoch: 3, Loss:  0.01526885386556387\n",
      "Easy dataset, Epoch: 3, Loss:  0.013070055283606052\n",
      "Easy dataset, Epoch: 4, Loss:  0.03366030752658844\n",
      "Easy dataset, Epoch: 4, Loss:  0.020824331790208817\n",
      "Easy dataset, Epoch: 4, Loss:  0.028336815536022186\n",
      "Easy dataset, Epoch: 4, Loss:  0.019679954275488853\n",
      "Easy dataset, Epoch: 4, Loss:  0.005053897388279438\n",
      "Easy dataset, Epoch: 4, Loss:  0.010211530141532421\n",
      "Easy dataset, Epoch: 4, Loss:  0.10556396096944809\n",
      "Easy dataset, Epoch: 4, Loss:  0.016642551869153976\n",
      "Easy dataset, Epoch: 4, Loss:  0.009739270433783531\n",
      "Easy dataset, Epoch: 4, Loss:  0.0015411395579576492\n",
      "Easy dataset, Epoch: 4, Loss:  0.01077947299927473\n",
      "Easy dataset, Epoch: 4, Loss:  0.004674892872571945\n",
      "Easy dataset, Epoch: 4, Loss:  0.15575261414051056\n",
      "Easy dataset, Epoch: 4, Loss:  0.018086135387420654\n",
      "Easy dataset, Epoch: 4, Loss:  0.001790822483599186\n",
      "Easy dataset, Epoch: 4, Loss:  0.009189946576952934\n",
      "Easy dataset, Epoch: 4, Loss:  0.22874362766742706\n",
      "Easy dataset, Epoch: 4, Loss:  0.07951671630144119\n",
      "Easy dataset, Epoch: 4, Loss:  0.007520847488194704\n",
      "Easy dataset, Epoch: 4, Loss:  0.034765854477882385\n",
      "Easy dataset, Epoch: 4, Loss:  0.00979454442858696\n",
      "Easy dataset, Epoch: 4, Loss:  0.15715451538562775\n",
      "Easy dataset, Epoch: 4, Loss:  0.007024393416941166\n",
      "Easy dataset, Epoch: 4, Loss:  0.009917736053466797\n",
      "Easy dataset, Epoch: 4, Loss:  0.010513199493288994\n",
      "Easy dataset, Epoch: 4, Loss:  0.07311186194419861\n",
      "Easy dataset, Epoch: 4, Loss:  0.008885134011507034\n",
      "Easy dataset, Epoch: 4, Loss:  0.0035189525224268436\n",
      "Easy dataset, Epoch: 4, Loss:  0.0030979865696281195\n",
      "Easy dataset, Epoch: 4, Loss:  0.03701793774962425\n",
      "Easy dataset, Epoch: 4, Loss:  0.007625633850693703\n",
      "Easy dataset, Epoch: 4, Loss:  0.013639677315950394\n",
      "Easy dataset, Epoch: 4, Loss:  0.0018504065228626132\n",
      "Easy dataset, Epoch: 4, Loss:  0.005567359738051891\n",
      "Easy dataset, Epoch: 4, Loss:  0.03858970105648041\n",
      "Easy dataset, Epoch: 4, Loss:  0.11763481795787811\n",
      "Easy dataset, Epoch: 4, Loss:  0.0016942010261118412\n",
      "Easy dataset, Epoch: 4, Loss:  0.005914755165576935\n",
      "Easy dataset, Epoch: 4, Loss:  0.0032014634925872087\n",
      "Easy dataset, Epoch: 4, Loss:  0.005183653440326452\n",
      "Easy dataset, Epoch: 4, Loss:  0.0015015811659395695\n",
      "Easy dataset, Epoch: 5, Loss:  0.004073581658303738\n",
      "Easy dataset, Epoch: 5, Loss:  0.006606223061680794\n",
      "Easy dataset, Epoch: 5, Loss:  0.22171372175216675\n",
      "Easy dataset, Epoch: 5, Loss:  0.0036203358322381973\n",
      "Easy dataset, Epoch: 5, Loss:  0.006796385161578655\n",
      "Easy dataset, Epoch: 5, Loss:  0.012995847500860691\n",
      "Easy dataset, Epoch: 5, Loss:  0.0033655765000730753\n",
      "Easy dataset, Epoch: 5, Loss:  0.03810421749949455\n",
      "Easy dataset, Epoch: 5, Loss:  0.005336393136531115\n",
      "Easy dataset, Epoch: 5, Loss:  0.10182163864374161\n",
      "Easy dataset, Epoch: 5, Loss:  0.14963999390602112\n",
      "Easy dataset, Epoch: 5, Loss:  0.011004679836332798\n",
      "Easy dataset, Epoch: 5, Loss:  0.0025628271978348494\n",
      "Easy dataset, Epoch: 5, Loss:  0.004224390722811222\n",
      "Easy dataset, Epoch: 5, Loss:  0.008117133751511574\n",
      "Easy dataset, Epoch: 5, Loss:  0.040564797818660736\n",
      "Easy dataset, Epoch: 5, Loss:  0.010533958673477173\n",
      "Easy dataset, Epoch: 5, Loss:  0.0020947803277522326\n",
      "Easy dataset, Epoch: 5, Loss:  0.010367575101554394\n",
      "Easy dataset, Epoch: 5, Loss:  0.001984576229006052\n",
      "Easy dataset, Epoch: 5, Loss:  0.010471520014107227\n",
      "Easy dataset, Epoch: 5, Loss:  0.041244663298130035\n",
      "Easy dataset, Epoch: 5, Loss:  0.00368152535520494\n",
      "Easy dataset, Epoch: 5, Loss:  0.005869750864803791\n",
      "Easy dataset, Epoch: 5, Loss:  0.0027600168250501156\n",
      "Easy dataset, Epoch: 5, Loss:  0.0013815329875797033\n",
      "Easy dataset, Epoch: 5, Loss:  0.002237895503640175\n",
      "Easy dataset, Epoch: 5, Loss:  0.0018641332862898707\n",
      "Easy dataset, Epoch: 5, Loss:  0.03203774616122246\n",
      "Easy dataset, Epoch: 5, Loss:  0.04110158979892731\n",
      "Easy dataset, Epoch: 5, Loss:  0.056137047708034515\n",
      "Easy dataset, Epoch: 5, Loss:  0.036921802908182144\n",
      "Easy dataset, Epoch: 5, Loss:  0.0022515172604471445\n",
      "Easy dataset, Epoch: 5, Loss:  0.019443411380052567\n",
      "Easy dataset, Epoch: 5, Loss:  0.05655030161142349\n",
      "Easy dataset, Epoch: 5, Loss:  0.1632302701473236\n",
      "Easy dataset, Epoch: 5, Loss:  0.003638425376266241\n",
      "Easy dataset, Epoch: 5, Loss:  0.005697551183402538\n",
      "Easy dataset, Epoch: 5, Loss:  0.015411032363772392\n",
      "Easy dataset, Epoch: 5, Loss:  0.007264442276209593\n",
      "Easy dataset, Epoch: 5, Loss:  0.0030196160078048706\n",
      "Easy dataset, Epoch: 6, Loss:  0.036760687828063965\n",
      "Easy dataset, Epoch: 6, Loss:  0.24923114478588104\n",
      "Easy dataset, Epoch: 6, Loss:  0.0014645857736468315\n",
      "Easy dataset, Epoch: 6, Loss:  0.001810451503843069\n",
      "Easy dataset, Epoch: 6, Loss:  0.011372310109436512\n",
      "Easy dataset, Epoch: 6, Loss:  0.0011495820945128798\n",
      "Easy dataset, Epoch: 6, Loss:  0.003313701134175062\n",
      "Easy dataset, Epoch: 6, Loss:  0.002224025782197714\n",
      "Easy dataset, Epoch: 6, Loss:  0.011174689047038555\n",
      "Easy dataset, Epoch: 6, Loss:  0.1206866204738617\n",
      "Easy dataset, Epoch: 6, Loss:  0.007815400138497353\n",
      "Easy dataset, Epoch: 6, Loss:  0.0021838201209902763\n",
      "Easy dataset, Epoch: 6, Loss:  0.016829105094075203\n",
      "Easy dataset, Epoch: 6, Loss:  0.07732615619897842\n",
      "Easy dataset, Epoch: 6, Loss:  0.004844730719923973\n",
      "Easy dataset, Epoch: 6, Loss:  0.06940381973981857\n",
      "Easy dataset, Epoch: 6, Loss:  0.03323976323008537\n",
      "Easy dataset, Epoch: 6, Loss:  0.004695875104516745\n",
      "Easy dataset, Epoch: 6, Loss:  0.0190125685185194\n",
      "Easy dataset, Epoch: 6, Loss:  0.005944902077317238\n",
      "Easy dataset, Epoch: 6, Loss:  0.0009775478392839432\n",
      "Easy dataset, Epoch: 6, Loss:  0.0063068196177482605\n",
      "Easy dataset, Epoch: 6, Loss:  0.0029287091456353664\n",
      "Easy dataset, Epoch: 6, Loss:  0.012088822200894356\n",
      "Easy dataset, Epoch: 6, Loss:  0.006429536268115044\n",
      "Easy dataset, Epoch: 6, Loss:  0.05062052607536316\n",
      "Easy dataset, Epoch: 6, Loss:  0.0012652184814214706\n",
      "Easy dataset, Epoch: 6, Loss:  0.029199443757534027\n",
      "Easy dataset, Epoch: 6, Loss:  0.07871126383543015\n",
      "Easy dataset, Epoch: 6, Loss:  0.0013462803326547146\n",
      "Easy dataset, Epoch: 6, Loss:  0.06486290693283081\n",
      "Easy dataset, Epoch: 6, Loss:  0.0057494789361953735\n",
      "Easy dataset, Epoch: 6, Loss:  0.08073289692401886\n",
      "Easy dataset, Epoch: 6, Loss:  0.007691724691540003\n",
      "Easy dataset, Epoch: 6, Loss:  0.006059741135686636\n",
      "Easy dataset, Epoch: 6, Loss:  0.002490496262907982\n",
      "Easy dataset, Epoch: 6, Loss:  0.015646982938051224\n",
      "Easy dataset, Epoch: 6, Loss:  0.0013680506963282824\n",
      "Easy dataset, Epoch: 6, Loss:  0.004771178588271141\n",
      "Easy dataset, Epoch: 6, Loss:  0.0013241793494671583\n",
      "Easy dataset, Epoch: 6, Loss:  0.08097139745950699\n",
      "Easy dataset, Epoch: 7, Loss:  0.021823517978191376\n",
      "Easy dataset, Epoch: 7, Loss:  0.006322560831904411\n",
      "Easy dataset, Epoch: 7, Loss:  0.009040086530148983\n",
      "Easy dataset, Epoch: 7, Loss:  0.003036213805899024\n",
      "Easy dataset, Epoch: 7, Loss:  0.11806102842092514\n",
      "Easy dataset, Epoch: 7, Loss:  0.043885376304388046\n",
      "Easy dataset, Epoch: 7, Loss:  0.0037989960983395576\n",
      "Easy dataset, Epoch: 7, Loss:  0.003423161804676056\n",
      "Easy dataset, Epoch: 7, Loss:  0.003453665878623724\n",
      "Easy dataset, Epoch: 7, Loss:  0.00218071136623621\n",
      "Easy dataset, Epoch: 7, Loss:  0.0026466127019375563\n",
      "Easy dataset, Epoch: 7, Loss:  0.0021904304157942533\n",
      "Easy dataset, Epoch: 7, Loss:  0.004657514858990908\n",
      "Easy dataset, Epoch: 7, Loss:  0.005883832462131977\n",
      "Easy dataset, Epoch: 7, Loss:  0.00831783190369606\n",
      "Easy dataset, Epoch: 7, Loss:  0.001974180107936263\n",
      "Easy dataset, Epoch: 7, Loss:  0.0023047607392072678\n",
      "Easy dataset, Epoch: 7, Loss:  0.0027101286686956882\n",
      "Easy dataset, Epoch: 7, Loss:  0.09486651420593262\n",
      "Easy dataset, Epoch: 7, Loss:  0.0010046365205198526\n",
      "Easy dataset, Epoch: 7, Loss:  0.0016424914356321096\n",
      "Easy dataset, Epoch: 7, Loss:  0.0027588391676545143\n",
      "Easy dataset, Epoch: 7, Loss:  0.005064868368208408\n",
      "Easy dataset, Epoch: 7, Loss:  0.0013040450867265463\n",
      "Easy dataset, Epoch: 7, Loss:  0.0036685247905552387\n",
      "Easy dataset, Epoch: 7, Loss:  0.0015046668704599142\n",
      "Easy dataset, Epoch: 7, Loss:  0.012159360572695732\n",
      "Easy dataset, Epoch: 7, Loss:  0.0018887551268562675\n",
      "Easy dataset, Epoch: 7, Loss:  0.007893869653344154\n",
      "Easy dataset, Epoch: 7, Loss:  0.0011247026268392801\n",
      "Easy dataset, Epoch: 7, Loss:  0.0075790961273014545\n",
      "Easy dataset, Epoch: 7, Loss:  0.0027035605162382126\n",
      "Easy dataset, Epoch: 7, Loss:  0.0022147803101688623\n",
      "Easy dataset, Epoch: 7, Loss:  0.001705075497739017\n",
      "Easy dataset, Epoch: 7, Loss:  0.0008781024371273816\n",
      "Easy dataset, Epoch: 7, Loss:  0.0009331986075267196\n",
      "Easy dataset, Epoch: 7, Loss:  0.000918887322768569\n",
      "Easy dataset, Epoch: 7, Loss:  0.003752068616449833\n",
      "Easy dataset, Epoch: 7, Loss:  0.004736651666462421\n",
      "Easy dataset, Epoch: 7, Loss:  0.0017173014348372817\n",
      "Easy dataset, Epoch: 7, Loss:  0.005692156031727791\n"
     ]
    }
   ],
   "source": [
    "# Start training for easy dataset\n",
    "for epoch in range(EPOCHS):\n",
    "    train(\"Easy\", easy_model, easy_training_loader, easy_optimizer, epoch)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medium dataset, Epoch: 0, Loss:  0.7261993288993835\n",
      "Medium dataset, Epoch: 0, Loss:  0.6842933297157288\n",
      "Medium dataset, Epoch: 0, Loss:  0.7016382217407227\n",
      "Medium dataset, Epoch: 0, Loss:  0.6757066249847412\n",
      "Medium dataset, Epoch: 0, Loss:  0.6748214960098267\n",
      "Medium dataset, Epoch: 0, Loss:  0.6900665760040283\n",
      "Medium dataset, Epoch: 0, Loss:  0.6605957746505737\n",
      "Medium dataset, Epoch: 0, Loss:  0.5572476387023926\n",
      "Medium dataset, Epoch: 0, Loss:  0.691226601600647\n",
      "Medium dataset, Epoch: 0, Loss:  0.599398136138916\n",
      "Medium dataset, Epoch: 0, Loss:  0.743026852607727\n",
      "Medium dataset, Epoch: 0, Loss:  0.651751697063446\n",
      "Medium dataset, Epoch: 0, Loss:  0.6179503202438354\n",
      "Medium dataset, Epoch: 0, Loss:  0.6795493960380554\n",
      "Medium dataset, Epoch: 0, Loss:  0.5032213926315308\n",
      "Medium dataset, Epoch: 0, Loss:  0.5402302742004395\n",
      "Medium dataset, Epoch: 0, Loss:  0.5252580046653748\n",
      "Medium dataset, Epoch: 0, Loss:  0.5791972875595093\n",
      "Medium dataset, Epoch: 0, Loss:  0.6022526025772095\n",
      "Medium dataset, Epoch: 0, Loss:  0.48382264375686646\n",
      "Medium dataset, Epoch: 0, Loss:  0.5566146373748779\n",
      "Medium dataset, Epoch: 0, Loss:  0.6084852814674377\n",
      "Medium dataset, Epoch: 0, Loss:  0.5133408308029175\n",
      "Medium dataset, Epoch: 0, Loss:  0.45992323756217957\n",
      "Medium dataset, Epoch: 0, Loss:  0.5483480095863342\n",
      "Medium dataset, Epoch: 0, Loss:  0.5577827095985413\n",
      "Medium dataset, Epoch: 0, Loss:  0.40698710083961487\n",
      "Medium dataset, Epoch: 0, Loss:  0.567292332649231\n",
      "Medium dataset, Epoch: 0, Loss:  0.5789136290550232\n",
      "Medium dataset, Epoch: 0, Loss:  0.594616174697876\n",
      "Medium dataset, Epoch: 0, Loss:  0.47564128041267395\n",
      "Medium dataset, Epoch: 0, Loss:  0.4623635411262512\n",
      "Medium dataset, Epoch: 0, Loss:  0.47578203678131104\n",
      "Medium dataset, Epoch: 0, Loss:  0.4884280860424042\n",
      "Medium dataset, Epoch: 0, Loss:  0.46800515055656433\n",
      "Medium dataset, Epoch: 0, Loss:  0.523689866065979\n",
      "Medium dataset, Epoch: 0, Loss:  0.5582120418548584\n",
      "Medium dataset, Epoch: 0, Loss:  0.39050817489624023\n",
      "Medium dataset, Epoch: 0, Loss:  0.3488829731941223\n",
      "Medium dataset, Epoch: 0, Loss:  0.5328344106674194\n",
      "Medium dataset, Epoch: 0, Loss:  0.41191285848617554\n",
      "Medium dataset, Epoch: 0, Loss:  0.576689600944519\n",
      "Medium dataset, Epoch: 0, Loss:  0.42527806758880615\n",
      "Medium dataset, Epoch: 0, Loss:  0.34362390637397766\n",
      "Medium dataset, Epoch: 0, Loss:  0.48718518018722534\n",
      "Medium dataset, Epoch: 0, Loss:  0.5568780899047852\n",
      "Medium dataset, Epoch: 0, Loss:  0.5810064077377319\n",
      "Medium dataset, Epoch: 0, Loss:  0.6101297736167908\n",
      "Medium dataset, Epoch: 0, Loss:  0.46800845861434937\n",
      "Medium dataset, Epoch: 0, Loss:  0.3726356029510498\n",
      "Medium dataset, Epoch: 0, Loss:  0.41981154680252075\n",
      "Medium dataset, Epoch: 0, Loss:  0.4090912342071533\n",
      "Medium dataset, Epoch: 0, Loss:  0.5191224813461304\n",
      "Medium dataset, Epoch: 0, Loss:  0.5935533046722412\n",
      "Medium dataset, Epoch: 0, Loss:  0.3983207046985626\n",
      "Medium dataset, Epoch: 0, Loss:  0.39130592346191406\n",
      "Medium dataset, Epoch: 0, Loss:  0.33151209354400635\n",
      "Medium dataset, Epoch: 0, Loss:  0.5891200304031372\n",
      "Medium dataset, Epoch: 0, Loss:  0.36960336565971375\n",
      "Medium dataset, Epoch: 0, Loss:  0.35744625329971313\n",
      "Medium dataset, Epoch: 0, Loss:  0.39406514167785645\n",
      "Medium dataset, Epoch: 0, Loss:  0.3262263536453247\n",
      "Medium dataset, Epoch: 0, Loss:  0.48375654220581055\n",
      "Medium dataset, Epoch: 0, Loss:  0.3838328719139099\n",
      "Medium dataset, Epoch: 0, Loss:  0.5240846872329712\n",
      "Medium dataset, Epoch: 0, Loss:  0.4059349596500397\n",
      "Medium dataset, Epoch: 0, Loss:  0.4749618470668793\n",
      "Medium dataset, Epoch: 0, Loss:  0.38940903544425964\n",
      "Medium dataset, Epoch: 0, Loss:  0.43657928705215454\n",
      "Medium dataset, Epoch: 0, Loss:  0.5430766940116882\n",
      "Medium dataset, Epoch: 0, Loss:  0.4970972537994385\n",
      "Medium dataset, Epoch: 0, Loss:  0.5774105191230774\n",
      "Medium dataset, Epoch: 0, Loss:  0.6026619672775269\n",
      "Medium dataset, Epoch: 0, Loss:  0.5009753704071045\n",
      "Medium dataset, Epoch: 0, Loss:  0.21753352880477905\n",
      "Medium dataset, Epoch: 0, Loss:  0.48413556814193726\n",
      "Medium dataset, Epoch: 0, Loss:  0.22688868641853333\n",
      "Medium dataset, Epoch: 0, Loss:  0.38096117973327637\n",
      "Medium dataset, Epoch: 0, Loss:  0.36272096633911133\n",
      "Medium dataset, Epoch: 0, Loss:  0.46494990587234497\n",
      "Medium dataset, Epoch: 0, Loss:  0.3915467858314514\n",
      "Medium dataset, Epoch: 0, Loss:  0.47625240683555603\n",
      "Medium dataset, Epoch: 0, Loss:  0.28326281905174255\n",
      "Medium dataset, Epoch: 0, Loss:  0.41058745980262756\n",
      "Medium dataset, Epoch: 0, Loss:  0.5142046213150024\n",
      "Medium dataset, Epoch: 0, Loss:  0.5234667658805847\n",
      "Medium dataset, Epoch: 0, Loss:  0.30401262640953064\n",
      "Medium dataset, Epoch: 0, Loss:  0.3263195753097534\n",
      "Medium dataset, Epoch: 0, Loss:  0.44394659996032715\n",
      "Medium dataset, Epoch: 1, Loss:  0.6291640996932983\n",
      "Medium dataset, Epoch: 1, Loss:  0.3467113971710205\n",
      "Medium dataset, Epoch: 1, Loss:  0.3766213059425354\n",
      "Medium dataset, Epoch: 1, Loss:  0.34274598956108093\n",
      "Medium dataset, Epoch: 1, Loss:  0.3538553714752197\n",
      "Medium dataset, Epoch: 1, Loss:  0.4450113773345947\n",
      "Medium dataset, Epoch: 1, Loss:  0.382700115442276\n",
      "Medium dataset, Epoch: 1, Loss:  0.4913727045059204\n",
      "Medium dataset, Epoch: 1, Loss:  0.5855666399002075\n",
      "Medium dataset, Epoch: 1, Loss:  0.543131947517395\n",
      "Medium dataset, Epoch: 1, Loss:  0.3816351592540741\n",
      "Medium dataset, Epoch: 1, Loss:  0.3414357304573059\n",
      "Medium dataset, Epoch: 1, Loss:  0.45083677768707275\n",
      "Medium dataset, Epoch: 1, Loss:  0.43846583366394043\n",
      "Medium dataset, Epoch: 1, Loss:  0.5592491030693054\n",
      "Medium dataset, Epoch: 1, Loss:  0.2492346167564392\n",
      "Medium dataset, Epoch: 1, Loss:  0.26095330715179443\n",
      "Medium dataset, Epoch: 1, Loss:  0.41781872510910034\n",
      "Medium dataset, Epoch: 1, Loss:  0.44889527559280396\n",
      "Medium dataset, Epoch: 1, Loss:  0.3737102746963501\n",
      "Medium dataset, Epoch: 1, Loss:  0.4584498107433319\n",
      "Medium dataset, Epoch: 1, Loss:  0.4582050144672394\n",
      "Medium dataset, Epoch: 1, Loss:  0.3480115830898285\n",
      "Medium dataset, Epoch: 1, Loss:  0.42648881673812866\n",
      "Medium dataset, Epoch: 1, Loss:  0.46589064598083496\n",
      "Medium dataset, Epoch: 1, Loss:  0.5202776193618774\n",
      "Medium dataset, Epoch: 1, Loss:  0.28693410754203796\n",
      "Medium dataset, Epoch: 1, Loss:  0.5303720831871033\n",
      "Medium dataset, Epoch: 1, Loss:  0.46219536662101746\n",
      "Medium dataset, Epoch: 1, Loss:  0.30355027318000793\n",
      "Medium dataset, Epoch: 1, Loss:  0.3187015652656555\n",
      "Medium dataset, Epoch: 1, Loss:  0.4132460355758667\n",
      "Medium dataset, Epoch: 1, Loss:  0.28866302967071533\n",
      "Medium dataset, Epoch: 1, Loss:  0.3066022992134094\n",
      "Medium dataset, Epoch: 1, Loss:  0.4142801761627197\n",
      "Medium dataset, Epoch: 1, Loss:  0.45989856123924255\n",
      "Medium dataset, Epoch: 1, Loss:  0.5295818448066711\n",
      "Medium dataset, Epoch: 1, Loss:  0.3217071294784546\n",
      "Medium dataset, Epoch: 1, Loss:  0.26839548349380493\n",
      "Medium dataset, Epoch: 1, Loss:  0.4822213053703308\n",
      "Medium dataset, Epoch: 1, Loss:  0.28313300013542175\n",
      "Medium dataset, Epoch: 1, Loss:  0.5257924795150757\n",
      "Medium dataset, Epoch: 1, Loss:  0.3827172517776489\n",
      "Medium dataset, Epoch: 1, Loss:  0.3883654475212097\n",
      "Medium dataset, Epoch: 1, Loss:  0.2395538091659546\n",
      "Medium dataset, Epoch: 1, Loss:  0.42483824491500854\n",
      "Medium dataset, Epoch: 1, Loss:  0.5141648650169373\n",
      "Medium dataset, Epoch: 1, Loss:  0.3417223393917084\n",
      "Medium dataset, Epoch: 1, Loss:  0.41421496868133545\n",
      "Medium dataset, Epoch: 1, Loss:  0.6254645586013794\n",
      "Medium dataset, Epoch: 1, Loss:  0.4998365044593811\n",
      "Medium dataset, Epoch: 1, Loss:  0.3289453387260437\n",
      "Medium dataset, Epoch: 1, Loss:  0.3477455973625183\n",
      "Medium dataset, Epoch: 1, Loss:  0.3701203465461731\n",
      "Medium dataset, Epoch: 1, Loss:  0.3371865749359131\n",
      "Medium dataset, Epoch: 1, Loss:  0.4718010425567627\n",
      "Medium dataset, Epoch: 1, Loss:  0.4090885818004608\n",
      "Medium dataset, Epoch: 1, Loss:  0.26929163932800293\n",
      "Medium dataset, Epoch: 1, Loss:  0.3651757538318634\n",
      "Medium dataset, Epoch: 1, Loss:  0.4125540256500244\n",
      "Medium dataset, Epoch: 1, Loss:  0.34584861993789673\n",
      "Medium dataset, Epoch: 1, Loss:  0.39876967668533325\n",
      "Medium dataset, Epoch: 1, Loss:  0.3368633985519409\n",
      "Medium dataset, Epoch: 1, Loss:  0.35180073976516724\n",
      "Medium dataset, Epoch: 1, Loss:  0.36729851365089417\n",
      "Medium dataset, Epoch: 1, Loss:  0.32890090346336365\n",
      "Medium dataset, Epoch: 1, Loss:  0.3002665638923645\n",
      "Medium dataset, Epoch: 1, Loss:  0.37012979388237\n",
      "Medium dataset, Epoch: 1, Loss:  0.30234891176223755\n",
      "Medium dataset, Epoch: 1, Loss:  0.27831941843032837\n",
      "Medium dataset, Epoch: 1, Loss:  0.23888087272644043\n",
      "Medium dataset, Epoch: 1, Loss:  0.38640111684799194\n",
      "Medium dataset, Epoch: 1, Loss:  0.45238763093948364\n",
      "Medium dataset, Epoch: 1, Loss:  0.358001172542572\n",
      "Medium dataset, Epoch: 1, Loss:  0.27856630086898804\n",
      "Medium dataset, Epoch: 1, Loss:  0.19151555001735687\n",
      "Medium dataset, Epoch: 1, Loss:  0.29649943113327026\n",
      "Medium dataset, Epoch: 1, Loss:  0.4611397385597229\n",
      "Medium dataset, Epoch: 1, Loss:  0.5673959255218506\n",
      "Medium dataset, Epoch: 1, Loss:  0.3405687212944031\n",
      "Medium dataset, Epoch: 1, Loss:  0.3547874987125397\n",
      "Medium dataset, Epoch: 1, Loss:  0.24671363830566406\n",
      "Medium dataset, Epoch: 1, Loss:  0.40587490797042847\n",
      "Medium dataset, Epoch: 1, Loss:  0.47232067584991455\n",
      "Medium dataset, Epoch: 1, Loss:  0.2594985365867615\n",
      "Medium dataset, Epoch: 1, Loss:  0.31873705983161926\n",
      "Medium dataset, Epoch: 1, Loss:  0.2853303849697113\n",
      "Medium dataset, Epoch: 1, Loss:  0.39510685205459595\n",
      "Medium dataset, Epoch: 1, Loss:  0.6918111443519592\n",
      "Medium dataset, Epoch: 2, Loss:  0.5049961805343628\n",
      "Medium dataset, Epoch: 2, Loss:  0.36621716618537903\n",
      "Medium dataset, Epoch: 2, Loss:  0.28398507833480835\n",
      "Medium dataset, Epoch: 2, Loss:  0.28595611453056335\n",
      "Medium dataset, Epoch: 2, Loss:  0.4042319059371948\n",
      "Medium dataset, Epoch: 2, Loss:  0.46256858110427856\n",
      "Medium dataset, Epoch: 2, Loss:  0.5525346994400024\n",
      "Medium dataset, Epoch: 2, Loss:  0.45404598116874695\n",
      "Medium dataset, Epoch: 2, Loss:  0.2996012270450592\n",
      "Medium dataset, Epoch: 2, Loss:  0.28569647669792175\n",
      "Medium dataset, Epoch: 2, Loss:  0.20263084769248962\n",
      "Medium dataset, Epoch: 2, Loss:  0.24240252375602722\n",
      "Medium dataset, Epoch: 2, Loss:  0.23808389902114868\n",
      "Medium dataset, Epoch: 2, Loss:  0.2946140766143799\n",
      "Medium dataset, Epoch: 2, Loss:  0.44809532165527344\n",
      "Medium dataset, Epoch: 2, Loss:  0.4284968376159668\n",
      "Medium dataset, Epoch: 2, Loss:  0.26480764150619507\n",
      "Medium dataset, Epoch: 2, Loss:  0.7219749093055725\n",
      "Medium dataset, Epoch: 2, Loss:  0.38340771198272705\n",
      "Medium dataset, Epoch: 2, Loss:  0.5223867297172546\n",
      "Medium dataset, Epoch: 2, Loss:  0.3557695150375366\n",
      "Medium dataset, Epoch: 2, Loss:  0.43461865186691284\n",
      "Medium dataset, Epoch: 2, Loss:  0.2586505115032196\n",
      "Medium dataset, Epoch: 2, Loss:  0.48273196816444397\n",
      "Medium dataset, Epoch: 2, Loss:  0.19642172753810883\n",
      "Medium dataset, Epoch: 2, Loss:  0.21005326509475708\n",
      "Medium dataset, Epoch: 2, Loss:  0.4093970060348511\n",
      "Medium dataset, Epoch: 2, Loss:  0.3689335584640503\n",
      "Medium dataset, Epoch: 2, Loss:  0.3179720938205719\n",
      "Medium dataset, Epoch: 2, Loss:  0.29881173372268677\n",
      "Medium dataset, Epoch: 2, Loss:  0.28866273164749146\n",
      "Medium dataset, Epoch: 2, Loss:  0.3448512852191925\n",
      "Medium dataset, Epoch: 2, Loss:  0.30947941541671753\n",
      "Medium dataset, Epoch: 2, Loss:  0.5613069534301758\n",
      "Medium dataset, Epoch: 2, Loss:  0.4457281529903412\n",
      "Medium dataset, Epoch: 2, Loss:  0.1968453824520111\n",
      "Medium dataset, Epoch: 2, Loss:  0.38952845335006714\n",
      "Medium dataset, Epoch: 2, Loss:  0.3783644437789917\n",
      "Medium dataset, Epoch: 2, Loss:  0.2197175920009613\n",
      "Medium dataset, Epoch: 2, Loss:  0.17329877614974976\n",
      "Medium dataset, Epoch: 2, Loss:  0.4031066298484802\n",
      "Medium dataset, Epoch: 2, Loss:  0.3353384733200073\n",
      "Medium dataset, Epoch: 2, Loss:  0.41721686720848083\n",
      "Medium dataset, Epoch: 2, Loss:  0.1412203460931778\n",
      "Medium dataset, Epoch: 2, Loss:  0.22529582679271698\n",
      "Medium dataset, Epoch: 2, Loss:  0.3616802990436554\n",
      "Medium dataset, Epoch: 2, Loss:  0.29483360052108765\n",
      "Medium dataset, Epoch: 2, Loss:  0.41696834564208984\n",
      "Medium dataset, Epoch: 2, Loss:  0.24316567182540894\n",
      "Medium dataset, Epoch: 2, Loss:  0.2921343147754669\n",
      "Medium dataset, Epoch: 2, Loss:  0.3035469055175781\n",
      "Medium dataset, Epoch: 2, Loss:  0.5326510071754456\n",
      "Medium dataset, Epoch: 2, Loss:  0.24863281846046448\n",
      "Medium dataset, Epoch: 2, Loss:  0.5013241767883301\n",
      "Medium dataset, Epoch: 2, Loss:  0.21151384711265564\n",
      "Medium dataset, Epoch: 2, Loss:  0.3468928039073944\n",
      "Medium dataset, Epoch: 2, Loss:  0.6075152158737183\n",
      "Medium dataset, Epoch: 2, Loss:  0.26124411821365356\n",
      "Medium dataset, Epoch: 2, Loss:  0.3198142647743225\n",
      "Medium dataset, Epoch: 2, Loss:  0.29277390241622925\n",
      "Medium dataset, Epoch: 2, Loss:  0.3255671262741089\n",
      "Medium dataset, Epoch: 2, Loss:  0.19208984076976776\n",
      "Medium dataset, Epoch: 2, Loss:  0.31908684968948364\n",
      "Medium dataset, Epoch: 2, Loss:  0.34151747822761536\n",
      "Medium dataset, Epoch: 2, Loss:  0.2839091122150421\n",
      "Medium dataset, Epoch: 2, Loss:  0.18789291381835938\n",
      "Medium dataset, Epoch: 2, Loss:  0.20604580640792847\n",
      "Medium dataset, Epoch: 2, Loss:  0.3720872402191162\n",
      "Medium dataset, Epoch: 2, Loss:  0.2217789888381958\n",
      "Medium dataset, Epoch: 2, Loss:  0.24459145963191986\n",
      "Medium dataset, Epoch: 2, Loss:  0.3012906312942505\n",
      "Medium dataset, Epoch: 2, Loss:  0.2516496181488037\n",
      "Medium dataset, Epoch: 2, Loss:  0.2120714783668518\n",
      "Medium dataset, Epoch: 2, Loss:  0.2753047049045563\n",
      "Medium dataset, Epoch: 2, Loss:  0.1902751922607422\n",
      "Medium dataset, Epoch: 2, Loss:  0.15500116348266602\n",
      "Medium dataset, Epoch: 2, Loss:  0.3734748959541321\n",
      "Medium dataset, Epoch: 2, Loss:  0.6543772220611572\n",
      "Medium dataset, Epoch: 2, Loss:  0.25455373525619507\n",
      "Medium dataset, Epoch: 2, Loss:  0.1409282088279724\n",
      "Medium dataset, Epoch: 2, Loss:  0.2948804497718811\n",
      "Medium dataset, Epoch: 2, Loss:  0.17521537840366364\n",
      "Medium dataset, Epoch: 2, Loss:  0.30925971269607544\n",
      "Medium dataset, Epoch: 2, Loss:  0.5307112336158752\n",
      "Medium dataset, Epoch: 2, Loss:  0.2173113226890564\n",
      "Medium dataset, Epoch: 2, Loss:  0.28498202562332153\n",
      "Medium dataset, Epoch: 2, Loss:  0.2685183584690094\n",
      "Medium dataset, Epoch: 2, Loss:  0.33489030599594116\n",
      "Medium dataset, Epoch: 2, Loss:  0.20704121887683868\n",
      "Medium dataset, Epoch: 3, Loss:  0.3445153832435608\n",
      "Medium dataset, Epoch: 3, Loss:  0.3473950922489166\n",
      "Medium dataset, Epoch: 3, Loss:  0.08120092749595642\n",
      "Medium dataset, Epoch: 3, Loss:  0.2960575520992279\n",
      "Medium dataset, Epoch: 3, Loss:  0.5421371459960938\n",
      "Medium dataset, Epoch: 3, Loss:  0.2500646412372589\n",
      "Medium dataset, Epoch: 3, Loss:  0.3592035472393036\n",
      "Medium dataset, Epoch: 3, Loss:  0.38184383511543274\n",
      "Medium dataset, Epoch: 3, Loss:  0.3653998076915741\n",
      "Medium dataset, Epoch: 3, Loss:  0.1907607913017273\n",
      "Medium dataset, Epoch: 3, Loss:  0.19846287369728088\n",
      "Medium dataset, Epoch: 3, Loss:  0.21171945333480835\n",
      "Medium dataset, Epoch: 3, Loss:  0.2243775725364685\n",
      "Medium dataset, Epoch: 3, Loss:  0.25092440843582153\n",
      "Medium dataset, Epoch: 3, Loss:  0.44066962599754333\n",
      "Medium dataset, Epoch: 3, Loss:  0.45351141691207886\n",
      "Medium dataset, Epoch: 3, Loss:  0.23161441087722778\n",
      "Medium dataset, Epoch: 3, Loss:  0.5426222085952759\n",
      "Medium dataset, Epoch: 3, Loss:  0.33859530091285706\n",
      "Medium dataset, Epoch: 3, Loss:  0.1702706217765808\n",
      "Medium dataset, Epoch: 3, Loss:  0.2592424750328064\n",
      "Medium dataset, Epoch: 3, Loss:  0.299116849899292\n",
      "Medium dataset, Epoch: 3, Loss:  0.18797825276851654\n",
      "Medium dataset, Epoch: 3, Loss:  0.4292893409729004\n",
      "Medium dataset, Epoch: 3, Loss:  0.31503862142562866\n",
      "Medium dataset, Epoch: 3, Loss:  0.220590278506279\n",
      "Medium dataset, Epoch: 3, Loss:  0.21588870882987976\n",
      "Medium dataset, Epoch: 3, Loss:  0.3083943724632263\n",
      "Medium dataset, Epoch: 3, Loss:  0.4296790063381195\n",
      "Medium dataset, Epoch: 3, Loss:  0.27753785252571106\n",
      "Medium dataset, Epoch: 3, Loss:  0.35299938917160034\n",
      "Medium dataset, Epoch: 3, Loss:  0.33051130175590515\n",
      "Medium dataset, Epoch: 3, Loss:  0.26032423973083496\n",
      "Medium dataset, Epoch: 3, Loss:  0.2778794765472412\n",
      "Medium dataset, Epoch: 3, Loss:  0.24150070548057556\n",
      "Medium dataset, Epoch: 3, Loss:  0.23497742414474487\n",
      "Medium dataset, Epoch: 3, Loss:  0.2916296124458313\n",
      "Medium dataset, Epoch: 3, Loss:  0.3780778646469116\n",
      "Medium dataset, Epoch: 3, Loss:  0.41271403431892395\n",
      "Medium dataset, Epoch: 3, Loss:  0.3030351996421814\n",
      "Medium dataset, Epoch: 3, Loss:  0.14572131633758545\n",
      "Medium dataset, Epoch: 3, Loss:  0.13346530497074127\n",
      "Medium dataset, Epoch: 3, Loss:  0.27059656381607056\n",
      "Medium dataset, Epoch: 3, Loss:  0.3113415837287903\n",
      "Medium dataset, Epoch: 3, Loss:  0.2040519118309021\n",
      "Medium dataset, Epoch: 3, Loss:  0.22167453169822693\n",
      "Medium dataset, Epoch: 3, Loss:  0.16395437717437744\n",
      "Medium dataset, Epoch: 3, Loss:  0.5088320970535278\n",
      "Medium dataset, Epoch: 3, Loss:  0.2448606789112091\n",
      "Medium dataset, Epoch: 3, Loss:  0.09757548570632935\n",
      "Medium dataset, Epoch: 3, Loss:  0.6502411365509033\n",
      "Medium dataset, Epoch: 3, Loss:  0.14671698212623596\n",
      "Medium dataset, Epoch: 3, Loss:  0.2141522765159607\n",
      "Medium dataset, Epoch: 3, Loss:  0.2984731197357178\n",
      "Medium dataset, Epoch: 3, Loss:  0.20830939710140228\n",
      "Medium dataset, Epoch: 3, Loss:  0.18143658339977264\n",
      "Medium dataset, Epoch: 3, Loss:  0.2770076096057892\n",
      "Medium dataset, Epoch: 3, Loss:  0.43014708161354065\n",
      "Medium dataset, Epoch: 3, Loss:  0.2387520968914032\n",
      "Medium dataset, Epoch: 3, Loss:  0.25696417689323425\n",
      "Medium dataset, Epoch: 3, Loss:  0.1779177039861679\n",
      "Medium dataset, Epoch: 3, Loss:  0.300809383392334\n",
      "Medium dataset, Epoch: 3, Loss:  0.18817779421806335\n",
      "Medium dataset, Epoch: 3, Loss:  0.41398313641548157\n",
      "Medium dataset, Epoch: 3, Loss:  0.15198837220668793\n",
      "Medium dataset, Epoch: 3, Loss:  0.3246714472770691\n",
      "Medium dataset, Epoch: 3, Loss:  0.24537301063537598\n",
      "Medium dataset, Epoch: 3, Loss:  0.14269624650478363\n",
      "Medium dataset, Epoch: 3, Loss:  0.033727314323186874\n",
      "Medium dataset, Epoch: 3, Loss:  0.27487844228744507\n",
      "Medium dataset, Epoch: 3, Loss:  0.2355646789073944\n",
      "Medium dataset, Epoch: 3, Loss:  0.26235759258270264\n",
      "Medium dataset, Epoch: 3, Loss:  0.5152135491371155\n",
      "Medium dataset, Epoch: 3, Loss:  0.31625470519065857\n",
      "Medium dataset, Epoch: 3, Loss:  0.1220114603638649\n",
      "Medium dataset, Epoch: 3, Loss:  0.3334794044494629\n",
      "Medium dataset, Epoch: 3, Loss:  0.34919339418411255\n",
      "Medium dataset, Epoch: 3, Loss:  0.047572530806064606\n",
      "Medium dataset, Epoch: 3, Loss:  0.08026352524757385\n",
      "Medium dataset, Epoch: 3, Loss:  0.19950111210346222\n",
      "Medium dataset, Epoch: 3, Loss:  0.22029098868370056\n",
      "Medium dataset, Epoch: 3, Loss:  0.2126740664243698\n",
      "Medium dataset, Epoch: 3, Loss:  0.4256369471549988\n",
      "Medium dataset, Epoch: 3, Loss:  0.2917770743370056\n",
      "Medium dataset, Epoch: 3, Loss:  0.06555333733558655\n",
      "Medium dataset, Epoch: 3, Loss:  0.25277775526046753\n",
      "Medium dataset, Epoch: 3, Loss:  0.3198443055152893\n",
      "Medium dataset, Epoch: 3, Loss:  0.46063974499702454\n",
      "Medium dataset, Epoch: 3, Loss:  0.2702483534812927\n",
      "Medium dataset, Epoch: 4, Loss:  0.34711211919784546\n",
      "Medium dataset, Epoch: 4, Loss:  0.17203855514526367\n",
      "Medium dataset, Epoch: 4, Loss:  0.2513512969017029\n",
      "Medium dataset, Epoch: 4, Loss:  0.22538432478904724\n",
      "Medium dataset, Epoch: 4, Loss:  0.4941372275352478\n",
      "Medium dataset, Epoch: 4, Loss:  0.20184442400932312\n",
      "Medium dataset, Epoch: 4, Loss:  0.1514723300933838\n",
      "Medium dataset, Epoch: 4, Loss:  0.18211103975772858\n",
      "Medium dataset, Epoch: 4, Loss:  0.17990480363368988\n",
      "Medium dataset, Epoch: 4, Loss:  0.1649913489818573\n",
      "Medium dataset, Epoch: 4, Loss:  0.26919910311698914\n",
      "Medium dataset, Epoch: 4, Loss:  0.36034566164016724\n",
      "Medium dataset, Epoch: 4, Loss:  0.08985026180744171\n",
      "Medium dataset, Epoch: 4, Loss:  0.11875514686107635\n",
      "Medium dataset, Epoch: 4, Loss:  0.23359891772270203\n",
      "Medium dataset, Epoch: 4, Loss:  0.26498523354530334\n",
      "Medium dataset, Epoch: 4, Loss:  0.15283066034317017\n",
      "Medium dataset, Epoch: 4, Loss:  0.13619597256183624\n",
      "Medium dataset, Epoch: 4, Loss:  0.39938727021217346\n",
      "Medium dataset, Epoch: 4, Loss:  0.2083006203174591\n",
      "Medium dataset, Epoch: 4, Loss:  0.15155887603759766\n",
      "Medium dataset, Epoch: 4, Loss:  0.2811381220817566\n",
      "Medium dataset, Epoch: 4, Loss:  0.4777423143386841\n",
      "Medium dataset, Epoch: 4, Loss:  0.2414514571428299\n",
      "Medium dataset, Epoch: 4, Loss:  0.25115686655044556\n",
      "Medium dataset, Epoch: 4, Loss:  0.1684015393257141\n",
      "Medium dataset, Epoch: 4, Loss:  0.2721364498138428\n",
      "Medium dataset, Epoch: 4, Loss:  0.21978655457496643\n",
      "Medium dataset, Epoch: 4, Loss:  0.1604773849248886\n",
      "Medium dataset, Epoch: 4, Loss:  0.19216690957546234\n",
      "Medium dataset, Epoch: 4, Loss:  0.11344292759895325\n",
      "Medium dataset, Epoch: 4, Loss:  0.21928593516349792\n",
      "Medium dataset, Epoch: 4, Loss:  0.46330493688583374\n",
      "Medium dataset, Epoch: 4, Loss:  0.1893828809261322\n",
      "Medium dataset, Epoch: 4, Loss:  0.21917545795440674\n",
      "Medium dataset, Epoch: 4, Loss:  0.20033667981624603\n",
      "Medium dataset, Epoch: 4, Loss:  0.3817930221557617\n",
      "Medium dataset, Epoch: 4, Loss:  0.24431422352790833\n",
      "Medium dataset, Epoch: 4, Loss:  0.1262601912021637\n",
      "Medium dataset, Epoch: 4, Loss:  0.2257232517004013\n",
      "Medium dataset, Epoch: 4, Loss:  0.2176363468170166\n",
      "Medium dataset, Epoch: 4, Loss:  0.2274428904056549\n",
      "Medium dataset, Epoch: 4, Loss:  0.21241484582424164\n",
      "Medium dataset, Epoch: 4, Loss:  0.17722460627555847\n",
      "Medium dataset, Epoch: 4, Loss:  0.19648043811321259\n",
      "Medium dataset, Epoch: 4, Loss:  0.3791183531284332\n",
      "Medium dataset, Epoch: 4, Loss:  0.2842887043952942\n",
      "Medium dataset, Epoch: 4, Loss:  0.24329867959022522\n",
      "Medium dataset, Epoch: 4, Loss:  0.22525270283222198\n",
      "Medium dataset, Epoch: 4, Loss:  0.33441269397735596\n",
      "Medium dataset, Epoch: 4, Loss:  0.1834014654159546\n",
      "Medium dataset, Epoch: 4, Loss:  0.32040148973464966\n",
      "Medium dataset, Epoch: 4, Loss:  0.08801407366991043\n",
      "Medium dataset, Epoch: 4, Loss:  0.18279844522476196\n",
      "Medium dataset, Epoch: 4, Loss:  0.2837347090244293\n",
      "Medium dataset, Epoch: 4, Loss:  0.17860901355743408\n",
      "Medium dataset, Epoch: 4, Loss:  0.31042906641960144\n",
      "Medium dataset, Epoch: 4, Loss:  0.19427034258842468\n",
      "Medium dataset, Epoch: 4, Loss:  0.15849563479423523\n",
      "Medium dataset, Epoch: 4, Loss:  0.08569078147411346\n",
      "Medium dataset, Epoch: 4, Loss:  0.11686575412750244\n",
      "Medium dataset, Epoch: 4, Loss:  0.09057144820690155\n",
      "Medium dataset, Epoch: 4, Loss:  0.08242449164390564\n",
      "Medium dataset, Epoch: 4, Loss:  0.09012188762426376\n",
      "Medium dataset, Epoch: 4, Loss:  0.1381896585226059\n",
      "Medium dataset, Epoch: 4, Loss:  0.07588907331228256\n",
      "Medium dataset, Epoch: 4, Loss:  0.44009754061698914\n",
      "Medium dataset, Epoch: 4, Loss:  0.10587592422962189\n",
      "Medium dataset, Epoch: 4, Loss:  0.2019045650959015\n",
      "Medium dataset, Epoch: 4, Loss:  0.0783175677061081\n",
      "Medium dataset, Epoch: 4, Loss:  0.3183755874633789\n",
      "Medium dataset, Epoch: 4, Loss:  0.21998564898967743\n",
      "Medium dataset, Epoch: 4, Loss:  0.10877840965986252\n",
      "Medium dataset, Epoch: 4, Loss:  0.06901690363883972\n",
      "Medium dataset, Epoch: 4, Loss:  0.20744562149047852\n",
      "Medium dataset, Epoch: 4, Loss:  0.1340317279100418\n",
      "Medium dataset, Epoch: 4, Loss:  0.12513120472431183\n",
      "Medium dataset, Epoch: 4, Loss:  0.15836533904075623\n",
      "Medium dataset, Epoch: 4, Loss:  0.14256666600704193\n",
      "Medium dataset, Epoch: 4, Loss:  0.17713792622089386\n",
      "Medium dataset, Epoch: 4, Loss:  0.15273454785346985\n",
      "Medium dataset, Epoch: 4, Loss:  0.16687022149562836\n",
      "Medium dataset, Epoch: 4, Loss:  0.30158644914627075\n",
      "Medium dataset, Epoch: 4, Loss:  0.28301432728767395\n",
      "Medium dataset, Epoch: 4, Loss:  0.17032542824745178\n",
      "Medium dataset, Epoch: 4, Loss:  0.024624746292829514\n",
      "Medium dataset, Epoch: 4, Loss:  0.21780043840408325\n",
      "Medium dataset, Epoch: 4, Loss:  0.07301867753267288\n",
      "Medium dataset, Epoch: 4, Loss:  0.17408712208271027\n",
      "Medium dataset, Epoch: 5, Loss:  0.15759161114692688\n",
      "Medium dataset, Epoch: 5, Loss:  0.14301522076129913\n",
      "Medium dataset, Epoch: 5, Loss:  0.2068701982498169\n",
      "Medium dataset, Epoch: 5, Loss:  0.3142450451850891\n",
      "Medium dataset, Epoch: 5, Loss:  0.16002072393894196\n",
      "Medium dataset, Epoch: 5, Loss:  0.17914167046546936\n",
      "Medium dataset, Epoch: 5, Loss:  0.10303734242916107\n",
      "Medium dataset, Epoch: 5, Loss:  0.2858184278011322\n",
      "Medium dataset, Epoch: 5, Loss:  0.1007796972990036\n",
      "Medium dataset, Epoch: 5, Loss:  0.1448913812637329\n",
      "Medium dataset, Epoch: 5, Loss:  0.3226737976074219\n",
      "Medium dataset, Epoch: 5, Loss:  0.2064298391342163\n",
      "Medium dataset, Epoch: 5, Loss:  0.1942063271999359\n",
      "Medium dataset, Epoch: 5, Loss:  0.18861782550811768\n",
      "Medium dataset, Epoch: 5, Loss:  0.10337316989898682\n",
      "Medium dataset, Epoch: 5, Loss:  0.1377527266740799\n",
      "Medium dataset, Epoch: 5, Loss:  0.09472125768661499\n",
      "Medium dataset, Epoch: 5, Loss:  0.05734149366617203\n",
      "Medium dataset, Epoch: 5, Loss:  0.11448915302753448\n",
      "Medium dataset, Epoch: 5, Loss:  0.037690695375204086\n",
      "Medium dataset, Epoch: 5, Loss:  0.20071908831596375\n",
      "Medium dataset, Epoch: 5, Loss:  0.19889703392982483\n",
      "Medium dataset, Epoch: 5, Loss:  0.32869791984558105\n",
      "Medium dataset, Epoch: 5, Loss:  0.37405627965927124\n",
      "Medium dataset, Epoch: 5, Loss:  0.11814340204000473\n",
      "Medium dataset, Epoch: 5, Loss:  0.18947432935237885\n",
      "Medium dataset, Epoch: 5, Loss:  0.23461434245109558\n",
      "Medium dataset, Epoch: 5, Loss:  0.08563340455293655\n",
      "Medium dataset, Epoch: 5, Loss:  0.24326325953006744\n",
      "Medium dataset, Epoch: 5, Loss:  0.11308702826499939\n",
      "Medium dataset, Epoch: 5, Loss:  0.1698113977909088\n",
      "Medium dataset, Epoch: 5, Loss:  0.0663737952709198\n",
      "Medium dataset, Epoch: 5, Loss:  0.1251804083585739\n",
      "Medium dataset, Epoch: 5, Loss:  0.12988834083080292\n",
      "Medium dataset, Epoch: 5, Loss:  0.2820621728897095\n",
      "Medium dataset, Epoch: 5, Loss:  0.5707154273986816\n",
      "Medium dataset, Epoch: 5, Loss:  0.1327170580625534\n",
      "Medium dataset, Epoch: 5, Loss:  0.02953885868191719\n",
      "Medium dataset, Epoch: 5, Loss:  0.3631840944290161\n",
      "Medium dataset, Epoch: 5, Loss:  0.10475587844848633\n",
      "Medium dataset, Epoch: 5, Loss:  0.23256894946098328\n",
      "Medium dataset, Epoch: 5, Loss:  0.1526031345129013\n",
      "Medium dataset, Epoch: 5, Loss:  0.28248831629753113\n",
      "Medium dataset, Epoch: 5, Loss:  0.11898881196975708\n",
      "Medium dataset, Epoch: 5, Loss:  0.29414528608322144\n",
      "Medium dataset, Epoch: 5, Loss:  0.07749129831790924\n",
      "Medium dataset, Epoch: 5, Loss:  0.051519736647605896\n",
      "Medium dataset, Epoch: 5, Loss:  0.06572946906089783\n",
      "Medium dataset, Epoch: 5, Loss:  0.27134716510772705\n",
      "Medium dataset, Epoch: 5, Loss:  0.08936545997858047\n",
      "Medium dataset, Epoch: 5, Loss:  0.36455976963043213\n",
      "Medium dataset, Epoch: 5, Loss:  0.31610432267189026\n",
      "Medium dataset, Epoch: 5, Loss:  0.18487273156642914\n",
      "Medium dataset, Epoch: 5, Loss:  0.3151862323284149\n",
      "Medium dataset, Epoch: 5, Loss:  0.11446468532085419\n",
      "Medium dataset, Epoch: 5, Loss:  0.05044633150100708\n",
      "Medium dataset, Epoch: 5, Loss:  0.1924617439508438\n",
      "Medium dataset, Epoch: 5, Loss:  0.16355928778648376\n",
      "Medium dataset, Epoch: 5, Loss:  0.11693139374256134\n",
      "Medium dataset, Epoch: 5, Loss:  0.15460485219955444\n",
      "Medium dataset, Epoch: 5, Loss:  0.08806121349334717\n",
      "Medium dataset, Epoch: 5, Loss:  0.047239307314157486\n",
      "Medium dataset, Epoch: 5, Loss:  0.1209757998585701\n",
      "Medium dataset, Epoch: 5, Loss:  0.06564590334892273\n",
      "Medium dataset, Epoch: 5, Loss:  0.3329660892486572\n",
      "Medium dataset, Epoch: 5, Loss:  0.2439793050289154\n",
      "Medium dataset, Epoch: 5, Loss:  0.20368030667304993\n",
      "Medium dataset, Epoch: 5, Loss:  0.2889537215232849\n",
      "Medium dataset, Epoch: 5, Loss:  0.2001699060201645\n",
      "Medium dataset, Epoch: 5, Loss:  0.03513861075043678\n",
      "Medium dataset, Epoch: 5, Loss:  0.3411652445793152\n",
      "Medium dataset, Epoch: 5, Loss:  0.11468733847141266\n",
      "Medium dataset, Epoch: 5, Loss:  0.22667747735977173\n",
      "Medium dataset, Epoch: 5, Loss:  0.13750644028186798\n",
      "Medium dataset, Epoch: 5, Loss:  0.28015008568763733\n",
      "Medium dataset, Epoch: 5, Loss:  0.12706556916236877\n",
      "Medium dataset, Epoch: 5, Loss:  0.27714329957962036\n",
      "Medium dataset, Epoch: 5, Loss:  0.18360105156898499\n",
      "Medium dataset, Epoch: 5, Loss:  0.014478511177003384\n",
      "Medium dataset, Epoch: 5, Loss:  0.17086590826511383\n",
      "Medium dataset, Epoch: 5, Loss:  0.19493471086025238\n",
      "Medium dataset, Epoch: 5, Loss:  0.25004786252975464\n",
      "Medium dataset, Epoch: 5, Loss:  0.10003945976495743\n",
      "Medium dataset, Epoch: 5, Loss:  0.15248654782772064\n",
      "Medium dataset, Epoch: 5, Loss:  0.10213267803192139\n",
      "Medium dataset, Epoch: 5, Loss:  0.14207100868225098\n",
      "Medium dataset, Epoch: 5, Loss:  0.12914253771305084\n",
      "Medium dataset, Epoch: 5, Loss:  0.34097597002983093\n",
      "Medium dataset, Epoch: 5, Loss:  0.19338425993919373\n",
      "Medium dataset, Epoch: 6, Loss:  0.07052934914827347\n",
      "Medium dataset, Epoch: 6, Loss:  0.10986407101154327\n",
      "Medium dataset, Epoch: 6, Loss:  0.1886633187532425\n",
      "Medium dataset, Epoch: 6, Loss:  0.13202233612537384\n",
      "Medium dataset, Epoch: 6, Loss:  0.15693821012973785\n",
      "Medium dataset, Epoch: 6, Loss:  0.17239496111869812\n",
      "Medium dataset, Epoch: 6, Loss:  0.12099909037351608\n",
      "Medium dataset, Epoch: 6, Loss:  0.19107425212860107\n",
      "Medium dataset, Epoch: 6, Loss:  0.08731094747781754\n",
      "Medium dataset, Epoch: 6, Loss:  0.2785651683807373\n",
      "Medium dataset, Epoch: 6, Loss:  0.14812658727169037\n",
      "Medium dataset, Epoch: 6, Loss:  0.29252275824546814\n",
      "Medium dataset, Epoch: 6, Loss:  0.11896578222513199\n",
      "Medium dataset, Epoch: 6, Loss:  0.10670500248670578\n",
      "Medium dataset, Epoch: 6, Loss:  0.08387254178524017\n",
      "Medium dataset, Epoch: 6, Loss:  0.07184772193431854\n",
      "Medium dataset, Epoch: 6, Loss:  0.14963139593601227\n",
      "Medium dataset, Epoch: 6, Loss:  0.3513117730617523\n",
      "Medium dataset, Epoch: 6, Loss:  0.20617756247520447\n",
      "Medium dataset, Epoch: 6, Loss:  0.07277713716030121\n",
      "Medium dataset, Epoch: 6, Loss:  0.3460485637187958\n",
      "Medium dataset, Epoch: 6, Loss:  0.11050406843423843\n",
      "Medium dataset, Epoch: 6, Loss:  0.07358036935329437\n",
      "Medium dataset, Epoch: 6, Loss:  0.10706828534603119\n",
      "Medium dataset, Epoch: 6, Loss:  0.03399747982621193\n",
      "Medium dataset, Epoch: 6, Loss:  0.03170979768037796\n",
      "Medium dataset, Epoch: 6, Loss:  0.17318099737167358\n",
      "Medium dataset, Epoch: 6, Loss:  0.30568617582321167\n",
      "Medium dataset, Epoch: 6, Loss:  0.20285937190055847\n",
      "Medium dataset, Epoch: 6, Loss:  0.06639174371957779\n",
      "Medium dataset, Epoch: 6, Loss:  0.051610223948955536\n",
      "Medium dataset, Epoch: 6, Loss:  0.05271423980593681\n",
      "Medium dataset, Epoch: 6, Loss:  0.05866529420018196\n",
      "Medium dataset, Epoch: 6, Loss:  0.2861182689666748\n",
      "Medium dataset, Epoch: 6, Loss:  0.033993057906627655\n",
      "Medium dataset, Epoch: 6, Loss:  0.07632320374250412\n",
      "Medium dataset, Epoch: 6, Loss:  0.23235419392585754\n",
      "Medium dataset, Epoch: 6, Loss:  0.11779625713825226\n",
      "Medium dataset, Epoch: 6, Loss:  0.18212491273880005\n",
      "Medium dataset, Epoch: 6, Loss:  0.050779253244400024\n",
      "Medium dataset, Epoch: 6, Loss:  0.25834402441978455\n",
      "Medium dataset, Epoch: 6, Loss:  0.1487952470779419\n",
      "Medium dataset, Epoch: 6, Loss:  0.24718055129051208\n",
      "Medium dataset, Epoch: 6, Loss:  0.3014262020587921\n",
      "Medium dataset, Epoch: 6, Loss:  0.06030288338661194\n",
      "Medium dataset, Epoch: 6, Loss:  0.2765814960002899\n",
      "Medium dataset, Epoch: 6, Loss:  0.33128970861434937\n",
      "Medium dataset, Epoch: 6, Loss:  0.2549298107624054\n",
      "Medium dataset, Epoch: 6, Loss:  0.0953991711139679\n",
      "Medium dataset, Epoch: 6, Loss:  0.06006252020597458\n",
      "Medium dataset, Epoch: 6, Loss:  0.25616180896759033\n",
      "Medium dataset, Epoch: 6, Loss:  0.04712768644094467\n",
      "Medium dataset, Epoch: 6, Loss:  0.15042908489704132\n",
      "Medium dataset, Epoch: 6, Loss:  0.11166797578334808\n",
      "Medium dataset, Epoch: 6, Loss:  0.047378506511449814\n",
      "Medium dataset, Epoch: 6, Loss:  0.024854060262441635\n",
      "Medium dataset, Epoch: 6, Loss:  0.07431414723396301\n",
      "Medium dataset, Epoch: 6, Loss:  0.05272630602121353\n",
      "Medium dataset, Epoch: 6, Loss:  0.04208960384130478\n",
      "Medium dataset, Epoch: 6, Loss:  0.1675632894039154\n",
      "Medium dataset, Epoch: 6, Loss:  0.3893912434577942\n",
      "Medium dataset, Epoch: 6, Loss:  0.12312212586402893\n",
      "Medium dataset, Epoch: 6, Loss:  0.08026129752397537\n",
      "Medium dataset, Epoch: 6, Loss:  0.1533328890800476\n",
      "Medium dataset, Epoch: 6, Loss:  0.16040769219398499\n",
      "Medium dataset, Epoch: 6, Loss:  0.033021003007888794\n",
      "Medium dataset, Epoch: 6, Loss:  0.08472228050231934\n",
      "Medium dataset, Epoch: 6, Loss:  0.06450758874416351\n",
      "Medium dataset, Epoch: 6, Loss:  0.08108893036842346\n",
      "Medium dataset, Epoch: 6, Loss:  0.08895537257194519\n",
      "Medium dataset, Epoch: 6, Loss:  0.12242226302623749\n",
      "Medium dataset, Epoch: 6, Loss:  0.0944189578294754\n",
      "Medium dataset, Epoch: 6, Loss:  0.10624627768993378\n",
      "Medium dataset, Epoch: 6, Loss:  0.0392397940158844\n",
      "Medium dataset, Epoch: 6, Loss:  0.23637235164642334\n",
      "Medium dataset, Epoch: 6, Loss:  0.25880903005599976\n",
      "Medium dataset, Epoch: 6, Loss:  0.16337940096855164\n",
      "Medium dataset, Epoch: 6, Loss:  0.18176469206809998\n",
      "Medium dataset, Epoch: 6, Loss:  0.3793913722038269\n",
      "Medium dataset, Epoch: 6, Loss:  0.30659836530685425\n",
      "Medium dataset, Epoch: 6, Loss:  0.10313650965690613\n",
      "Medium dataset, Epoch: 6, Loss:  0.03606482595205307\n",
      "Medium dataset, Epoch: 6, Loss:  0.12252512574195862\n",
      "Medium dataset, Epoch: 6, Loss:  0.18057604134082794\n",
      "Medium dataset, Epoch: 6, Loss:  0.11291222274303436\n",
      "Medium dataset, Epoch: 6, Loss:  0.2607722282409668\n",
      "Medium dataset, Epoch: 6, Loss:  0.0402892529964447\n",
      "Medium dataset, Epoch: 6, Loss:  0.1504189521074295\n",
      "Medium dataset, Epoch: 6, Loss:  0.050887126475572586\n",
      "Medium dataset, Epoch: 7, Loss:  0.27566567063331604\n",
      "Medium dataset, Epoch: 7, Loss:  0.13502195477485657\n",
      "Medium dataset, Epoch: 7, Loss:  0.0490899495780468\n",
      "Medium dataset, Epoch: 7, Loss:  0.046921998262405396\n",
      "Medium dataset, Epoch: 7, Loss:  0.2262835055589676\n",
      "Medium dataset, Epoch: 7, Loss:  0.05626966431736946\n",
      "Medium dataset, Epoch: 7, Loss:  0.1642192006111145\n",
      "Medium dataset, Epoch: 7, Loss:  0.1372361034154892\n",
      "Medium dataset, Epoch: 7, Loss:  0.014293339103460312\n",
      "Medium dataset, Epoch: 7, Loss:  0.1330139935016632\n",
      "Medium dataset, Epoch: 7, Loss:  0.21031171083450317\n",
      "Medium dataset, Epoch: 7, Loss:  0.09210171550512314\n",
      "Medium dataset, Epoch: 7, Loss:  0.03340635821223259\n",
      "Medium dataset, Epoch: 7, Loss:  0.05693496763706207\n",
      "Medium dataset, Epoch: 7, Loss:  0.06210413575172424\n",
      "Medium dataset, Epoch: 7, Loss:  0.038335785269737244\n",
      "Medium dataset, Epoch: 7, Loss:  0.19376111030578613\n",
      "Medium dataset, Epoch: 7, Loss:  0.3167405128479004\n",
      "Medium dataset, Epoch: 7, Loss:  0.15467019379138947\n",
      "Medium dataset, Epoch: 7, Loss:  0.052832670509815216\n",
      "Medium dataset, Epoch: 7, Loss:  0.107240229845047\n",
      "Medium dataset, Epoch: 7, Loss:  0.10266725718975067\n",
      "Medium dataset, Epoch: 7, Loss:  0.03683143109083176\n",
      "Medium dataset, Epoch: 7, Loss:  0.13096323609352112\n",
      "Medium dataset, Epoch: 7, Loss:  0.17784778773784637\n",
      "Medium dataset, Epoch: 7, Loss:  0.06449703127145767\n",
      "Medium dataset, Epoch: 7, Loss:  0.07836982607841492\n",
      "Medium dataset, Epoch: 7, Loss:  0.10570870339870453\n",
      "Medium dataset, Epoch: 7, Loss:  0.1242704838514328\n",
      "Medium dataset, Epoch: 7, Loss:  0.1360601782798767\n",
      "Medium dataset, Epoch: 7, Loss:  0.10293561965227127\n",
      "Medium dataset, Epoch: 7, Loss:  0.05772341787815094\n",
      "Medium dataset, Epoch: 7, Loss:  0.010888155549764633\n",
      "Medium dataset, Epoch: 7, Loss:  0.02274041250348091\n",
      "Medium dataset, Epoch: 7, Loss:  0.09387005120515823\n",
      "Medium dataset, Epoch: 7, Loss:  0.19583095610141754\n",
      "Medium dataset, Epoch: 7, Loss:  0.4061186909675598\n",
      "Medium dataset, Epoch: 7, Loss:  0.011854318901896477\n",
      "Medium dataset, Epoch: 7, Loss:  0.045554570853710175\n",
      "Medium dataset, Epoch: 7, Loss:  0.18859821557998657\n",
      "Medium dataset, Epoch: 7, Loss:  0.03256713226437569\n",
      "Medium dataset, Epoch: 7, Loss:  0.1510869264602661\n",
      "Medium dataset, Epoch: 7, Loss:  0.293756365776062\n",
      "Medium dataset, Epoch: 7, Loss:  0.09284651279449463\n",
      "Medium dataset, Epoch: 7, Loss:  0.2850169539451599\n",
      "Medium dataset, Epoch: 7, Loss:  0.057306356728076935\n",
      "Medium dataset, Epoch: 7, Loss:  0.031095651909708977\n",
      "Medium dataset, Epoch: 7, Loss:  0.17324580252170563\n",
      "Medium dataset, Epoch: 7, Loss:  0.09634833037853241\n",
      "Medium dataset, Epoch: 7, Loss:  0.045105211436748505\n",
      "Medium dataset, Epoch: 7, Loss:  0.11305608600378036\n",
      "Medium dataset, Epoch: 7, Loss:  0.09409863501787186\n",
      "Medium dataset, Epoch: 7, Loss:  0.19500002264976501\n",
      "Medium dataset, Epoch: 7, Loss:  0.030492648482322693\n",
      "Medium dataset, Epoch: 7, Loss:  0.012640511617064476\n",
      "Medium dataset, Epoch: 7, Loss:  0.06821993738412857\n",
      "Medium dataset, Epoch: 7, Loss:  0.09749463945627213\n",
      "Medium dataset, Epoch: 7, Loss:  0.07003241777420044\n",
      "Medium dataset, Epoch: 7, Loss:  0.11519867926836014\n",
      "Medium dataset, Epoch: 7, Loss:  0.042599454522132874\n",
      "Medium dataset, Epoch: 7, Loss:  0.07946591079235077\n",
      "Medium dataset, Epoch: 7, Loss:  0.24384404718875885\n",
      "Medium dataset, Epoch: 7, Loss:  0.2310120314359665\n",
      "Medium dataset, Epoch: 7, Loss:  0.01599443331360817\n",
      "Medium dataset, Epoch: 7, Loss:  0.04602322727441788\n",
      "Medium dataset, Epoch: 7, Loss:  0.010630492120981216\n",
      "Medium dataset, Epoch: 7, Loss:  0.035612840205430984\n",
      "Medium dataset, Epoch: 7, Loss:  0.01605219393968582\n",
      "Medium dataset, Epoch: 7, Loss:  0.014334945008158684\n",
      "Medium dataset, Epoch: 7, Loss:  0.015644289553165436\n",
      "Medium dataset, Epoch: 7, Loss:  0.06466040015220642\n",
      "Medium dataset, Epoch: 7, Loss:  0.01758575811982155\n",
      "Medium dataset, Epoch: 7, Loss:  0.04562649130821228\n",
      "Medium dataset, Epoch: 7, Loss:  0.18392816185951233\n",
      "Medium dataset, Epoch: 7, Loss:  0.1614171713590622\n",
      "Medium dataset, Epoch: 7, Loss:  0.03403640165925026\n",
      "Medium dataset, Epoch: 7, Loss:  0.0376301109790802\n",
      "Medium dataset, Epoch: 7, Loss:  0.18392324447631836\n",
      "Medium dataset, Epoch: 7, Loss:  0.05875531956553459\n",
      "Medium dataset, Epoch: 7, Loss:  0.16546623408794403\n",
      "Medium dataset, Epoch: 7, Loss:  0.05894816294312477\n",
      "Medium dataset, Epoch: 7, Loss:  0.29131361842155457\n",
      "Medium dataset, Epoch: 7, Loss:  0.01574360579252243\n",
      "Medium dataset, Epoch: 7, Loss:  0.024867281317710876\n",
      "Medium dataset, Epoch: 7, Loss:  0.07371913641691208\n",
      "Medium dataset, Epoch: 7, Loss:  0.031366266310214996\n",
      "Medium dataset, Epoch: 7, Loss:  0.14672143757343292\n",
      "Medium dataset, Epoch: 7, Loss:  0.02767971344292164\n",
      "Medium dataset, Epoch: 7, Loss:  0.13981294631958008\n"
     ]
    }
   ],
   "source": [
    "# Start training for medium dataset\n",
    "for epoch in range(EPOCHS):\n",
    "    train(\"Medium\", medium_model, medium_training_loader, medium_optimizer, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\sonic\\Desktop\\a\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hard dataset, Epoch: 0, Loss:  0.69991135597229\n",
      "Hard dataset, Epoch: 0, Loss:  0.6875680685043335\n",
      "Hard dataset, Epoch: 0, Loss:  0.6750866174697876\n",
      "Hard dataset, Epoch: 0, Loss:  0.7164607644081116\n",
      "Hard dataset, Epoch: 0, Loss:  0.692764401435852\n",
      "Hard dataset, Epoch: 0, Loss:  0.6873019933700562\n",
      "Hard dataset, Epoch: 0, Loss:  0.6840401887893677\n",
      "Hard dataset, Epoch: 0, Loss:  0.6561689376831055\n",
      "Hard dataset, Epoch: 0, Loss:  0.7137378454208374\n",
      "Hard dataset, Epoch: 0, Loss:  0.6998664736747742\n",
      "Hard dataset, Epoch: 0, Loss:  0.6876404285430908\n",
      "Hard dataset, Epoch: 0, Loss:  0.6967781782150269\n",
      "Hard dataset, Epoch: 0, Loss:  0.6689280271530151\n",
      "Hard dataset, Epoch: 0, Loss:  0.7008069753646851\n",
      "Hard dataset, Epoch: 0, Loss:  0.6713554859161377\n",
      "Hard dataset, Epoch: 0, Loss:  0.6972436904907227\n",
      "Hard dataset, Epoch: 0, Loss:  0.6772338151931763\n",
      "Hard dataset, Epoch: 0, Loss:  0.690706729888916\n",
      "Hard dataset, Epoch: 0, Loss:  0.6620197296142578\n",
      "Hard dataset, Epoch: 0, Loss:  0.6989880800247192\n",
      "Hard dataset, Epoch: 0, Loss:  0.65888512134552\n",
      "Hard dataset, Epoch: 0, Loss:  0.655472993850708\n",
      "Hard dataset, Epoch: 0, Loss:  0.6992628574371338\n",
      "Hard dataset, Epoch: 0, Loss:  0.6907747387886047\n",
      "Hard dataset, Epoch: 0, Loss:  0.6728551387786865\n",
      "Hard dataset, Epoch: 0, Loss:  0.6656875014305115\n",
      "Hard dataset, Epoch: 0, Loss:  0.7447293400764465\n",
      "Hard dataset, Epoch: 0, Loss:  0.7309359312057495\n",
      "Hard dataset, Epoch: 0, Loss:  0.6937605142593384\n",
      "Hard dataset, Epoch: 0, Loss:  0.6730939149856567\n",
      "Hard dataset, Epoch: 0, Loss:  0.6525332927703857\n",
      "Hard dataset, Epoch: 0, Loss:  0.6552039384841919\n",
      "Hard dataset, Epoch: 0, Loss:  0.7220096588134766\n",
      "Hard dataset, Epoch: 0, Loss:  0.9515987634658813\n",
      "Hard dataset, Epoch: 0, Loss:  0.6709011793136597\n",
      "Hard dataset, Epoch: 0, Loss:  0.6455398797988892\n",
      "Hard dataset, Epoch: 0, Loss:  0.6218202710151672\n",
      "Hard dataset, Epoch: 0, Loss:  0.7094888687133789\n",
      "Hard dataset, Epoch: 0, Loss:  0.57469242811203\n",
      "Hard dataset, Epoch: 0, Loss:  0.6279935836791992\n",
      "Hard dataset, Epoch: 0, Loss:  0.5742734670639038\n",
      "Hard dataset, Epoch: 0, Loss:  0.6690387725830078\n",
      "Hard dataset, Epoch: 0, Loss:  0.58558189868927\n",
      "Hard dataset, Epoch: 0, Loss:  0.5636930465698242\n",
      "Hard dataset, Epoch: 0, Loss:  0.585191011428833\n",
      "Hard dataset, Epoch: 0, Loss:  0.5700811743736267\n",
      "Hard dataset, Epoch: 0, Loss:  0.7902039289474487\n",
      "Hard dataset, Epoch: 0, Loss:  0.710303544998169\n",
      "Hard dataset, Epoch: 0, Loss:  0.594241201877594\n",
      "Hard dataset, Epoch: 0, Loss:  0.587181568145752\n",
      "Hard dataset, Epoch: 0, Loss:  0.5913270115852356\n",
      "Hard dataset, Epoch: 0, Loss:  0.6519478559494019\n",
      "Hard dataset, Epoch: 0, Loss:  0.6983543634414673\n",
      "Hard dataset, Epoch: 0, Loss:  0.5355101227760315\n",
      "Hard dataset, Epoch: 0, Loss:  0.6702202558517456\n",
      "Hard dataset, Epoch: 0, Loss:  0.7469475269317627\n",
      "Hard dataset, Epoch: 0, Loss:  0.5341846942901611\n",
      "Hard dataset, Epoch: 0, Loss:  0.6570554971694946\n",
      "Hard dataset, Epoch: 0, Loss:  0.6758044958114624\n",
      "Hard dataset, Epoch: 0, Loss:  0.5869584679603577\n",
      "Hard dataset, Epoch: 1, Loss:  0.7840589284896851\n",
      "Hard dataset, Epoch: 1, Loss:  0.6291122436523438\n",
      "Hard dataset, Epoch: 1, Loss:  0.4739623963832855\n",
      "Hard dataset, Epoch: 1, Loss:  0.7850211262702942\n",
      "Hard dataset, Epoch: 1, Loss:  0.5339890718460083\n",
      "Hard dataset, Epoch: 1, Loss:  0.41027987003326416\n",
      "Hard dataset, Epoch: 1, Loss:  0.4201396107673645\n",
      "Hard dataset, Epoch: 1, Loss:  0.4618312120437622\n",
      "Hard dataset, Epoch: 1, Loss:  0.397962361574173\n",
      "Hard dataset, Epoch: 1, Loss:  0.5755066871643066\n",
      "Hard dataset, Epoch: 1, Loss:  0.7276090979576111\n",
      "Hard dataset, Epoch: 1, Loss:  0.38037705421447754\n",
      "Hard dataset, Epoch: 1, Loss:  0.6986109018325806\n",
      "Hard dataset, Epoch: 1, Loss:  0.5753624439239502\n",
      "Hard dataset, Epoch: 1, Loss:  0.625741720199585\n",
      "Hard dataset, Epoch: 1, Loss:  0.5166071057319641\n",
      "Hard dataset, Epoch: 1, Loss:  0.48484182357788086\n",
      "Hard dataset, Epoch: 1, Loss:  0.45779305696487427\n",
      "Hard dataset, Epoch: 1, Loss:  0.4228968918323517\n",
      "Hard dataset, Epoch: 1, Loss:  0.6366022825241089\n",
      "Hard dataset, Epoch: 1, Loss:  0.4810323119163513\n",
      "Hard dataset, Epoch: 1, Loss:  0.5240089893341064\n",
      "Hard dataset, Epoch: 1, Loss:  0.41634589433670044\n",
      "Hard dataset, Epoch: 1, Loss:  0.4875063896179199\n",
      "Hard dataset, Epoch: 1, Loss:  0.5346684455871582\n",
      "Hard dataset, Epoch: 1, Loss:  0.4117176830768585\n",
      "Hard dataset, Epoch: 1, Loss:  0.539577841758728\n",
      "Hard dataset, Epoch: 1, Loss:  0.3397524356842041\n",
      "Hard dataset, Epoch: 1, Loss:  0.4324284791946411\n",
      "Hard dataset, Epoch: 1, Loss:  0.6553422212600708\n",
      "Hard dataset, Epoch: 1, Loss:  0.37564945220947266\n",
      "Hard dataset, Epoch: 1, Loss:  0.46651506423950195\n",
      "Hard dataset, Epoch: 1, Loss:  0.5230855941772461\n",
      "Hard dataset, Epoch: 1, Loss:  0.2371860295534134\n",
      "Hard dataset, Epoch: 1, Loss:  0.4941220283508301\n",
      "Hard dataset, Epoch: 1, Loss:  0.6400532722473145\n",
      "Hard dataset, Epoch: 1, Loss:  0.44836145639419556\n",
      "Hard dataset, Epoch: 1, Loss:  0.3793482184410095\n",
      "Hard dataset, Epoch: 1, Loss:  0.6146202087402344\n",
      "Hard dataset, Epoch: 1, Loss:  0.3568670153617859\n",
      "Hard dataset, Epoch: 1, Loss:  0.3601972460746765\n",
      "Hard dataset, Epoch: 1, Loss:  0.5087620615959167\n",
      "Hard dataset, Epoch: 1, Loss:  0.4417944550514221\n",
      "Hard dataset, Epoch: 1, Loss:  0.5061935186386108\n",
      "Hard dataset, Epoch: 1, Loss:  0.4679769277572632\n",
      "Hard dataset, Epoch: 1, Loss:  0.3083934485912323\n",
      "Hard dataset, Epoch: 1, Loss:  0.5276851058006287\n",
      "Hard dataset, Epoch: 1, Loss:  0.8384081125259399\n",
      "Hard dataset, Epoch: 1, Loss:  0.47989946603775024\n",
      "Hard dataset, Epoch: 1, Loss:  0.3553788661956787\n",
      "Hard dataset, Epoch: 1, Loss:  0.38849666714668274\n",
      "Hard dataset, Epoch: 1, Loss:  0.3446984887123108\n",
      "Hard dataset, Epoch: 1, Loss:  0.4431607127189636\n",
      "Hard dataset, Epoch: 1, Loss:  0.5069024562835693\n",
      "Hard dataset, Epoch: 1, Loss:  0.4110700488090515\n",
      "Hard dataset, Epoch: 1, Loss:  0.398743212223053\n",
      "Hard dataset, Epoch: 1, Loss:  0.676032543182373\n",
      "Hard dataset, Epoch: 1, Loss:  0.45433279871940613\n",
      "Hard dataset, Epoch: 1, Loss:  0.4313768446445465\n",
      "Hard dataset, Epoch: 1, Loss:  0.3489830493927002\n",
      "Hard dataset, Epoch: 2, Loss:  0.25053197145462036\n",
      "Hard dataset, Epoch: 2, Loss:  0.25911945104599\n",
      "Hard dataset, Epoch: 2, Loss:  0.23987923562526703\n",
      "Hard dataset, Epoch: 2, Loss:  0.18235380947589874\n",
      "Hard dataset, Epoch: 2, Loss:  0.4160134792327881\n",
      "Hard dataset, Epoch: 2, Loss:  0.4379653334617615\n",
      "Hard dataset, Epoch: 2, Loss:  0.4107949435710907\n",
      "Hard dataset, Epoch: 2, Loss:  0.5065178871154785\n",
      "Hard dataset, Epoch: 2, Loss:  0.2913382053375244\n",
      "Hard dataset, Epoch: 2, Loss:  0.48201900720596313\n",
      "Hard dataset, Epoch: 2, Loss:  0.3511970639228821\n",
      "Hard dataset, Epoch: 2, Loss:  0.5648929476737976\n",
      "Hard dataset, Epoch: 2, Loss:  0.42307335138320923\n",
      "Hard dataset, Epoch: 2, Loss:  0.44092708826065063\n",
      "Hard dataset, Epoch: 2, Loss:  0.3198760747909546\n",
      "Hard dataset, Epoch: 2, Loss:  0.6597551107406616\n",
      "Hard dataset, Epoch: 2, Loss:  0.2917005717754364\n",
      "Hard dataset, Epoch: 2, Loss:  0.33841121196746826\n",
      "Hard dataset, Epoch: 2, Loss:  0.45544975996017456\n",
      "Hard dataset, Epoch: 2, Loss:  0.47222262620925903\n",
      "Hard dataset, Epoch: 2, Loss:  0.41936516761779785\n",
      "Hard dataset, Epoch: 2, Loss:  0.22462916374206543\n",
      "Hard dataset, Epoch: 2, Loss:  0.2452087700366974\n",
      "Hard dataset, Epoch: 2, Loss:  0.15476195514202118\n",
      "Hard dataset, Epoch: 2, Loss:  0.3712434768676758\n",
      "Hard dataset, Epoch: 2, Loss:  0.3355233371257782\n",
      "Hard dataset, Epoch: 2, Loss:  0.2886050343513489\n",
      "Hard dataset, Epoch: 2, Loss:  0.38333165645599365\n",
      "Hard dataset, Epoch: 2, Loss:  0.41969504952430725\n",
      "Hard dataset, Epoch: 2, Loss:  0.5961912870407104\n",
      "Hard dataset, Epoch: 2, Loss:  0.3465312123298645\n",
      "Hard dataset, Epoch: 2, Loss:  0.5384751558303833\n",
      "Hard dataset, Epoch: 2, Loss:  0.2797962427139282\n",
      "Hard dataset, Epoch: 2, Loss:  0.3695458471775055\n",
      "Hard dataset, Epoch: 2, Loss:  0.250376433134079\n",
      "Hard dataset, Epoch: 2, Loss:  0.27823472023010254\n",
      "Hard dataset, Epoch: 2, Loss:  0.42055216431617737\n",
      "Hard dataset, Epoch: 2, Loss:  0.3291093409061432\n",
      "Hard dataset, Epoch: 2, Loss:  0.3016475439071655\n",
      "Hard dataset, Epoch: 2, Loss:  0.4357590973377228\n",
      "Hard dataset, Epoch: 2, Loss:  0.3362314999103546\n",
      "Hard dataset, Epoch: 2, Loss:  0.5223996043205261\n",
      "Hard dataset, Epoch: 2, Loss:  0.3060018718242645\n",
      "Hard dataset, Epoch: 2, Loss:  0.47786945104599\n",
      "Hard dataset, Epoch: 2, Loss:  0.3434193730354309\n",
      "Hard dataset, Epoch: 2, Loss:  0.2760133147239685\n",
      "Hard dataset, Epoch: 2, Loss:  0.3744186758995056\n",
      "Hard dataset, Epoch: 2, Loss:  0.381586492061615\n",
      "Hard dataset, Epoch: 2, Loss:  0.4406282305717468\n",
      "Hard dataset, Epoch: 2, Loss:  0.3464597761631012\n",
      "Hard dataset, Epoch: 2, Loss:  0.29035496711730957\n",
      "Hard dataset, Epoch: 2, Loss:  0.4144759774208069\n",
      "Hard dataset, Epoch: 2, Loss:  0.4238508939743042\n",
      "Hard dataset, Epoch: 2, Loss:  0.4845883846282959\n",
      "Hard dataset, Epoch: 2, Loss:  0.36893942952156067\n",
      "Hard dataset, Epoch: 2, Loss:  0.28692594170570374\n",
      "Hard dataset, Epoch: 2, Loss:  0.2851657569408417\n",
      "Hard dataset, Epoch: 2, Loss:  0.30232930183410645\n",
      "Hard dataset, Epoch: 2, Loss:  0.4250074028968811\n",
      "Hard dataset, Epoch: 2, Loss:  0.4901992082595825\n",
      "Hard dataset, Epoch: 3, Loss:  0.27688273787498474\n",
      "Hard dataset, Epoch: 3, Loss:  0.2816767394542694\n",
      "Hard dataset, Epoch: 3, Loss:  0.32082241773605347\n",
      "Hard dataset, Epoch: 3, Loss:  0.2602483332157135\n",
      "Hard dataset, Epoch: 3, Loss:  0.21582327783107758\n",
      "Hard dataset, Epoch: 3, Loss:  0.2433689534664154\n",
      "Hard dataset, Epoch: 3, Loss:  0.17696654796600342\n",
      "Hard dataset, Epoch: 3, Loss:  0.29230520129203796\n",
      "Hard dataset, Epoch: 3, Loss:  0.4006577730178833\n",
      "Hard dataset, Epoch: 3, Loss:  0.3265923857688904\n",
      "Hard dataset, Epoch: 3, Loss:  0.41245439648628235\n",
      "Hard dataset, Epoch: 3, Loss:  0.2103569209575653\n",
      "Hard dataset, Epoch: 3, Loss:  0.2281695455312729\n",
      "Hard dataset, Epoch: 3, Loss:  0.15071940422058105\n",
      "Hard dataset, Epoch: 3, Loss:  0.3190634548664093\n",
      "Hard dataset, Epoch: 3, Loss:  0.5423969626426697\n",
      "Hard dataset, Epoch: 3, Loss:  0.28361260890960693\n",
      "Hard dataset, Epoch: 3, Loss:  0.2762454152107239\n",
      "Hard dataset, Epoch: 3, Loss:  0.3016836643218994\n",
      "Hard dataset, Epoch: 3, Loss:  0.12379114329814911\n",
      "Hard dataset, Epoch: 3, Loss:  0.26339346170425415\n",
      "Hard dataset, Epoch: 3, Loss:  0.4841592609882355\n",
      "Hard dataset, Epoch: 3, Loss:  0.2962098717689514\n",
      "Hard dataset, Epoch: 3, Loss:  0.3865191638469696\n",
      "Hard dataset, Epoch: 3, Loss:  0.316220223903656\n",
      "Hard dataset, Epoch: 3, Loss:  0.23982994258403778\n",
      "Hard dataset, Epoch: 3, Loss:  0.17527450621128082\n",
      "Hard dataset, Epoch: 3, Loss:  0.1449993997812271\n",
      "Hard dataset, Epoch: 3, Loss:  0.32527169585227966\n",
      "Hard dataset, Epoch: 3, Loss:  0.18277516961097717\n",
      "Hard dataset, Epoch: 3, Loss:  0.21199893951416016\n",
      "Hard dataset, Epoch: 3, Loss:  0.18642884492874146\n",
      "Hard dataset, Epoch: 3, Loss:  0.3238159418106079\n",
      "Hard dataset, Epoch: 3, Loss:  0.32167550921440125\n",
      "Hard dataset, Epoch: 3, Loss:  0.47720497846603394\n",
      "Hard dataset, Epoch: 3, Loss:  0.3234776258468628\n",
      "Hard dataset, Epoch: 3, Loss:  0.2515455484390259\n",
      "Hard dataset, Epoch: 3, Loss:  0.356285035610199\n",
      "Hard dataset, Epoch: 3, Loss:  0.33272498846054077\n",
      "Hard dataset, Epoch: 3, Loss:  0.355241984128952\n",
      "Hard dataset, Epoch: 3, Loss:  0.1570429801940918\n",
      "Hard dataset, Epoch: 3, Loss:  0.2285390943288803\n",
      "Hard dataset, Epoch: 3, Loss:  0.2298087775707245\n",
      "Hard dataset, Epoch: 3, Loss:  0.11167658120393753\n",
      "Hard dataset, Epoch: 3, Loss:  0.5607633590698242\n",
      "Hard dataset, Epoch: 3, Loss:  0.29093265533447266\n",
      "Hard dataset, Epoch: 3, Loss:  0.35021811723709106\n",
      "Hard dataset, Epoch: 3, Loss:  0.3989018499851227\n",
      "Hard dataset, Epoch: 3, Loss:  0.21950531005859375\n",
      "Hard dataset, Epoch: 3, Loss:  0.20064927637577057\n",
      "Hard dataset, Epoch: 3, Loss:  0.17846989631652832\n",
      "Hard dataset, Epoch: 3, Loss:  0.20969976484775543\n",
      "Hard dataset, Epoch: 3, Loss:  0.3524249792098999\n",
      "Hard dataset, Epoch: 3, Loss:  0.1929132044315338\n",
      "Hard dataset, Epoch: 3, Loss:  0.13354696333408356\n",
      "Hard dataset, Epoch: 3, Loss:  0.40518128871917725\n",
      "Hard dataset, Epoch: 3, Loss:  0.19035795331001282\n",
      "Hard dataset, Epoch: 3, Loss:  0.499153733253479\n",
      "Hard dataset, Epoch: 3, Loss:  0.38755789399147034\n",
      "Hard dataset, Epoch: 3, Loss:  0.3292595148086548\n",
      "Hard dataset, Epoch: 4, Loss:  0.18019288778305054\n",
      "Hard dataset, Epoch: 4, Loss:  0.1955145299434662\n",
      "Hard dataset, Epoch: 4, Loss:  0.10970757901668549\n",
      "Hard dataset, Epoch: 4, Loss:  0.40840035676956177\n",
      "Hard dataset, Epoch: 4, Loss:  0.16609951853752136\n",
      "Hard dataset, Epoch: 4, Loss:  0.4681059420108795\n",
      "Hard dataset, Epoch: 4, Loss:  0.23265978693962097\n",
      "Hard dataset, Epoch: 4, Loss:  0.3097970187664032\n",
      "Hard dataset, Epoch: 4, Loss:  0.2924773395061493\n",
      "Hard dataset, Epoch: 4, Loss:  0.45985597372055054\n",
      "Hard dataset, Epoch: 4, Loss:  0.06165969371795654\n",
      "Hard dataset, Epoch: 4, Loss:  0.4261942505836487\n",
      "Hard dataset, Epoch: 4, Loss:  0.2648005485534668\n",
      "Hard dataset, Epoch: 4, Loss:  0.21578171849250793\n",
      "Hard dataset, Epoch: 4, Loss:  0.16576576232910156\n",
      "Hard dataset, Epoch: 4, Loss:  0.16477882862091064\n",
      "Hard dataset, Epoch: 4, Loss:  0.2734115719795227\n",
      "Hard dataset, Epoch: 4, Loss:  0.30435195565223694\n",
      "Hard dataset, Epoch: 4, Loss:  0.3792533874511719\n",
      "Hard dataset, Epoch: 4, Loss:  0.18617069721221924\n",
      "Hard dataset, Epoch: 4, Loss:  0.41229236125946045\n",
      "Hard dataset, Epoch: 4, Loss:  0.09285000711679459\n",
      "Hard dataset, Epoch: 4, Loss:  0.18015384674072266\n",
      "Hard dataset, Epoch: 4, Loss:  0.18542152643203735\n",
      "Hard dataset, Epoch: 4, Loss:  0.12304310500621796\n",
      "Hard dataset, Epoch: 4, Loss:  0.129036083817482\n",
      "Hard dataset, Epoch: 4, Loss:  0.3794930577278137\n",
      "Hard dataset, Epoch: 4, Loss:  0.13862863183021545\n",
      "Hard dataset, Epoch: 4, Loss:  0.19113150238990784\n",
      "Hard dataset, Epoch: 4, Loss:  0.18567273020744324\n",
      "Hard dataset, Epoch: 4, Loss:  0.2512405514717102\n",
      "Hard dataset, Epoch: 4, Loss:  0.4098612070083618\n",
      "Hard dataset, Epoch: 4, Loss:  0.23348671197891235\n",
      "Hard dataset, Epoch: 4, Loss:  0.41447311639785767\n",
      "Hard dataset, Epoch: 4, Loss:  0.2180531919002533\n",
      "Hard dataset, Epoch: 4, Loss:  0.14911240339279175\n",
      "Hard dataset, Epoch: 4, Loss:  0.3506202697753906\n",
      "Hard dataset, Epoch: 4, Loss:  0.23513400554656982\n",
      "Hard dataset, Epoch: 4, Loss:  0.18696627020835876\n",
      "Hard dataset, Epoch: 4, Loss:  0.0773855596780777\n",
      "Hard dataset, Epoch: 4, Loss:  0.09509676694869995\n",
      "Hard dataset, Epoch: 4, Loss:  0.28774237632751465\n",
      "Hard dataset, Epoch: 4, Loss:  0.2991843521595001\n",
      "Hard dataset, Epoch: 4, Loss:  0.3133147954940796\n",
      "Hard dataset, Epoch: 4, Loss:  0.28308355808258057\n",
      "Hard dataset, Epoch: 4, Loss:  0.08676626533269882\n",
      "Hard dataset, Epoch: 4, Loss:  0.16125987470149994\n",
      "Hard dataset, Epoch: 4, Loss:  0.07179226726293564\n",
      "Hard dataset, Epoch: 4, Loss:  0.19464850425720215\n",
      "Hard dataset, Epoch: 4, Loss:  0.19247685372829437\n",
      "Hard dataset, Epoch: 4, Loss:  0.26693034172058105\n",
      "Hard dataset, Epoch: 4, Loss:  0.4093652069568634\n",
      "Hard dataset, Epoch: 4, Loss:  0.2815325856208801\n",
      "Hard dataset, Epoch: 4, Loss:  0.14265486598014832\n",
      "Hard dataset, Epoch: 4, Loss:  0.150319904088974\n",
      "Hard dataset, Epoch: 4, Loss:  0.14662213623523712\n",
      "Hard dataset, Epoch: 4, Loss:  0.12972410023212433\n",
      "Hard dataset, Epoch: 4, Loss:  0.10870786756277084\n",
      "Hard dataset, Epoch: 4, Loss:  0.11116845160722733\n",
      "Hard dataset, Epoch: 4, Loss:  0.16705110669136047\n",
      "Hard dataset, Epoch: 5, Loss:  0.1954697072505951\n",
      "Hard dataset, Epoch: 5, Loss:  0.3143967390060425\n",
      "Hard dataset, Epoch: 5, Loss:  0.1421436369419098\n",
      "Hard dataset, Epoch: 5, Loss:  0.1575981080532074\n",
      "Hard dataset, Epoch: 5, Loss:  0.31535008549690247\n",
      "Hard dataset, Epoch: 5, Loss:  0.16056355834007263\n",
      "Hard dataset, Epoch: 5, Loss:  0.48493099212646484\n",
      "Hard dataset, Epoch: 5, Loss:  0.3582186698913574\n",
      "Hard dataset, Epoch: 5, Loss:  0.27696898579597473\n",
      "Hard dataset, Epoch: 5, Loss:  0.2616664469242096\n",
      "Hard dataset, Epoch: 5, Loss:  0.14599072933197021\n",
      "Hard dataset, Epoch: 5, Loss:  0.09401081502437592\n",
      "Hard dataset, Epoch: 5, Loss:  0.10484319925308228\n",
      "Hard dataset, Epoch: 5, Loss:  0.1438044309616089\n",
      "Hard dataset, Epoch: 5, Loss:  0.12464442849159241\n",
      "Hard dataset, Epoch: 5, Loss:  0.13637879490852356\n",
      "Hard dataset, Epoch: 5, Loss:  0.13880935311317444\n",
      "Hard dataset, Epoch: 5, Loss:  0.07259713113307953\n",
      "Hard dataset, Epoch: 5, Loss:  0.19176732003688812\n",
      "Hard dataset, Epoch: 5, Loss:  0.10820943862199783\n",
      "Hard dataset, Epoch: 5, Loss:  0.11798712611198425\n",
      "Hard dataset, Epoch: 5, Loss:  0.28885072469711304\n",
      "Hard dataset, Epoch: 5, Loss:  0.1091671884059906\n",
      "Hard dataset, Epoch: 5, Loss:  0.09363391995429993\n",
      "Hard dataset, Epoch: 5, Loss:  0.1641947478055954\n",
      "Hard dataset, Epoch: 5, Loss:  0.09066188335418701\n",
      "Hard dataset, Epoch: 5, Loss:  0.09055987745523453\n",
      "Hard dataset, Epoch: 5, Loss:  0.1085655465722084\n",
      "Hard dataset, Epoch: 5, Loss:  0.1911933273077011\n",
      "Hard dataset, Epoch: 5, Loss:  0.14499294757843018\n",
      "Hard dataset, Epoch: 5, Loss:  0.31097450852394104\n",
      "Hard dataset, Epoch: 5, Loss:  0.15540707111358643\n",
      "Hard dataset, Epoch: 5, Loss:  0.16369694471359253\n",
      "Hard dataset, Epoch: 5, Loss:  0.10519558191299438\n",
      "Hard dataset, Epoch: 5, Loss:  0.07256980240345001\n",
      "Hard dataset, Epoch: 5, Loss:  0.22887691855430603\n",
      "Hard dataset, Epoch: 5, Loss:  0.23478230834007263\n",
      "Hard dataset, Epoch: 5, Loss:  0.08181071281433105\n",
      "Hard dataset, Epoch: 5, Loss:  0.22472387552261353\n",
      "Hard dataset, Epoch: 5, Loss:  0.1881633996963501\n",
      "Hard dataset, Epoch: 5, Loss:  0.19455988705158234\n",
      "Hard dataset, Epoch: 5, Loss:  0.23038625717163086\n",
      "Hard dataset, Epoch: 5, Loss:  0.042700350284576416\n",
      "Hard dataset, Epoch: 5, Loss:  0.20089194178581238\n",
      "Hard dataset, Epoch: 5, Loss:  0.036908507347106934\n",
      "Hard dataset, Epoch: 5, Loss:  0.064334437251091\n",
      "Hard dataset, Epoch: 5, Loss:  0.09115640819072723\n",
      "Hard dataset, Epoch: 5, Loss:  0.11876644939184189\n",
      "Hard dataset, Epoch: 5, Loss:  0.028218938037753105\n",
      "Hard dataset, Epoch: 5, Loss:  0.20827683806419373\n",
      "Hard dataset, Epoch: 5, Loss:  0.4635884165763855\n",
      "Hard dataset, Epoch: 5, Loss:  0.3435739576816559\n",
      "Hard dataset, Epoch: 5, Loss:  0.1375831514596939\n",
      "Hard dataset, Epoch: 5, Loss:  0.11711825430393219\n",
      "Hard dataset, Epoch: 5, Loss:  0.24800235033035278\n",
      "Hard dataset, Epoch: 5, Loss:  0.12915346026420593\n",
      "Hard dataset, Epoch: 5, Loss:  0.08748818933963776\n",
      "Hard dataset, Epoch: 5, Loss:  0.36048561334609985\n",
      "Hard dataset, Epoch: 5, Loss:  0.1110304743051529\n",
      "Hard dataset, Epoch: 5, Loss:  0.09410960972309113\n",
      "Hard dataset, Epoch: 6, Loss:  0.03269128128886223\n",
      "Hard dataset, Epoch: 6, Loss:  0.10777164250612259\n",
      "Hard dataset, Epoch: 6, Loss:  0.09850074350833893\n",
      "Hard dataset, Epoch: 6, Loss:  0.3504279553890228\n",
      "Hard dataset, Epoch: 6, Loss:  0.09294314682483673\n",
      "Hard dataset, Epoch: 6, Loss:  0.15175582468509674\n",
      "Hard dataset, Epoch: 6, Loss:  0.019272956997156143\n",
      "Hard dataset, Epoch: 6, Loss:  0.10769166797399521\n",
      "Hard dataset, Epoch: 6, Loss:  0.11208903044462204\n",
      "Hard dataset, Epoch: 6, Loss:  0.08666662871837616\n",
      "Hard dataset, Epoch: 6, Loss:  0.05192768573760986\n",
      "Hard dataset, Epoch: 6, Loss:  0.03449494391679764\n",
      "Hard dataset, Epoch: 6, Loss:  0.32514387369155884\n",
      "Hard dataset, Epoch: 6, Loss:  0.2789779603481293\n",
      "Hard dataset, Epoch: 6, Loss:  0.18490354716777802\n",
      "Hard dataset, Epoch: 6, Loss:  0.14048200845718384\n",
      "Hard dataset, Epoch: 6, Loss:  0.2242399901151657\n",
      "Hard dataset, Epoch: 6, Loss:  0.17787916958332062\n",
      "Hard dataset, Epoch: 6, Loss:  0.13957814872264862\n",
      "Hard dataset, Epoch: 6, Loss:  0.3822990655899048\n",
      "Hard dataset, Epoch: 6, Loss:  0.058579765260219574\n",
      "Hard dataset, Epoch: 6, Loss:  0.12122641503810883\n",
      "Hard dataset, Epoch: 6, Loss:  0.2625138461589813\n",
      "Hard dataset, Epoch: 6, Loss:  0.1200370341539383\n",
      "Hard dataset, Epoch: 6, Loss:  0.08691450208425522\n",
      "Hard dataset, Epoch: 6, Loss:  0.30771714448928833\n",
      "Hard dataset, Epoch: 6, Loss:  0.18558534979820251\n",
      "Hard dataset, Epoch: 6, Loss:  0.03489691764116287\n",
      "Hard dataset, Epoch: 6, Loss:  0.054209619760513306\n",
      "Hard dataset, Epoch: 6, Loss:  0.03133302181959152\n",
      "Hard dataset, Epoch: 6, Loss:  0.14769800007343292\n",
      "Hard dataset, Epoch: 6, Loss:  0.09016930311918259\n",
      "Hard dataset, Epoch: 6, Loss:  0.10861634463071823\n",
      "Hard dataset, Epoch: 6, Loss:  0.29369840025901794\n",
      "Hard dataset, Epoch: 6, Loss:  0.0645296573638916\n",
      "Hard dataset, Epoch: 6, Loss:  0.24164745211601257\n",
      "Hard dataset, Epoch: 6, Loss:  0.10364585369825363\n",
      "Hard dataset, Epoch: 6, Loss:  0.05567498132586479\n",
      "Hard dataset, Epoch: 6, Loss:  0.15757571160793304\n",
      "Hard dataset, Epoch: 6, Loss:  0.1437021642923355\n",
      "Hard dataset, Epoch: 6, Loss:  0.06056791543960571\n",
      "Hard dataset, Epoch: 6, Loss:  0.15588544309139252\n",
      "Hard dataset, Epoch: 6, Loss:  0.23069526255130768\n",
      "Hard dataset, Epoch: 6, Loss:  0.05675816163420677\n",
      "Hard dataset, Epoch: 6, Loss:  0.15260180830955505\n",
      "Hard dataset, Epoch: 6, Loss:  0.06854349374771118\n",
      "Hard dataset, Epoch: 6, Loss:  0.07079875469207764\n",
      "Hard dataset, Epoch: 6, Loss:  0.03253299742937088\n",
      "Hard dataset, Epoch: 6, Loss:  0.10785423219203949\n",
      "Hard dataset, Epoch: 6, Loss:  0.033497631549835205\n",
      "Hard dataset, Epoch: 6, Loss:  0.02315530553460121\n",
      "Hard dataset, Epoch: 6, Loss:  0.03000963106751442\n",
      "Hard dataset, Epoch: 6, Loss:  0.14719684422016144\n",
      "Hard dataset, Epoch: 6, Loss:  0.12670178711414337\n",
      "Hard dataset, Epoch: 6, Loss:  0.11451809853315353\n",
      "Hard dataset, Epoch: 6, Loss:  0.07983984798192978\n",
      "Hard dataset, Epoch: 6, Loss:  0.04338741675019264\n",
      "Hard dataset, Epoch: 6, Loss:  0.526788055896759\n",
      "Hard dataset, Epoch: 6, Loss:  0.16630640625953674\n",
      "Hard dataset, Epoch: 6, Loss:  0.05039600655436516\n",
      "Hard dataset, Epoch: 7, Loss:  0.03399939090013504\n",
      "Hard dataset, Epoch: 7, Loss:  0.07887119054794312\n",
      "Hard dataset, Epoch: 7, Loss:  0.13141627609729767\n",
      "Hard dataset, Epoch: 7, Loss:  0.06957817822694778\n",
      "Hard dataset, Epoch: 7, Loss:  0.11194705963134766\n",
      "Hard dataset, Epoch: 7, Loss:  0.19907408952713013\n",
      "Hard dataset, Epoch: 7, Loss:  0.1260817050933838\n",
      "Hard dataset, Epoch: 7, Loss:  0.19833770394325256\n",
      "Hard dataset, Epoch: 7, Loss:  0.09610571712255478\n",
      "Hard dataset, Epoch: 7, Loss:  0.21281586587429047\n",
      "Hard dataset, Epoch: 7, Loss:  0.02767832949757576\n",
      "Hard dataset, Epoch: 7, Loss:  0.154434934258461\n",
      "Hard dataset, Epoch: 7, Loss:  0.011124669574201107\n",
      "Hard dataset, Epoch: 7, Loss:  0.09086550027132034\n",
      "Hard dataset, Epoch: 7, Loss:  0.04591860622167587\n",
      "Hard dataset, Epoch: 7, Loss:  0.1514129340648651\n",
      "Hard dataset, Epoch: 7, Loss:  0.27373626828193665\n",
      "Hard dataset, Epoch: 7, Loss:  0.01083204336464405\n",
      "Hard dataset, Epoch: 7, Loss:  0.00695900060236454\n",
      "Hard dataset, Epoch: 7, Loss:  0.07027631998062134\n",
      "Hard dataset, Epoch: 7, Loss:  0.1019800454378128\n",
      "Hard dataset, Epoch: 7, Loss:  0.1114964485168457\n",
      "Hard dataset, Epoch: 7, Loss:  0.19091495871543884\n",
      "Hard dataset, Epoch: 7, Loss:  0.04986052215099335\n",
      "Hard dataset, Epoch: 7, Loss:  0.18483151495456696\n",
      "Hard dataset, Epoch: 7, Loss:  0.04413433372974396\n",
      "Hard dataset, Epoch: 7, Loss:  0.05770786106586456\n",
      "Hard dataset, Epoch: 7, Loss:  0.2444753795862198\n",
      "Hard dataset, Epoch: 7, Loss:  0.10053853690624237\n",
      "Hard dataset, Epoch: 7, Loss:  0.011163299903273582\n",
      "Hard dataset, Epoch: 7, Loss:  0.27010899782180786\n",
      "Hard dataset, Epoch: 7, Loss:  0.07436234503984451\n",
      "Hard dataset, Epoch: 7, Loss:  0.015468025580048561\n",
      "Hard dataset, Epoch: 7, Loss:  0.12816257774829865\n",
      "Hard dataset, Epoch: 7, Loss:  0.13747915625572205\n",
      "Hard dataset, Epoch: 7, Loss:  0.1961948573589325\n",
      "Hard dataset, Epoch: 7, Loss:  0.16788943111896515\n",
      "Hard dataset, Epoch: 7, Loss:  0.03229258954524994\n",
      "Hard dataset, Epoch: 7, Loss:  0.18553201854228973\n",
      "Hard dataset, Epoch: 7, Loss:  0.0638166293501854\n",
      "Hard dataset, Epoch: 7, Loss:  0.20155617594718933\n",
      "Hard dataset, Epoch: 7, Loss:  0.2032037079334259\n",
      "Hard dataset, Epoch: 7, Loss:  0.01307961530983448\n",
      "Hard dataset, Epoch: 7, Loss:  0.017573285847902298\n",
      "Hard dataset, Epoch: 7, Loss:  0.09708824008703232\n",
      "Hard dataset, Epoch: 7, Loss:  0.18392644822597504\n",
      "Hard dataset, Epoch: 7, Loss:  0.04384700581431389\n",
      "Hard dataset, Epoch: 7, Loss:  0.07085877656936646\n",
      "Hard dataset, Epoch: 7, Loss:  0.0873989388346672\n",
      "Hard dataset, Epoch: 7, Loss:  0.2730576992034912\n",
      "Hard dataset, Epoch: 7, Loss:  0.1447908580303192\n",
      "Hard dataset, Epoch: 7, Loss:  0.091828353703022\n",
      "Hard dataset, Epoch: 7, Loss:  0.04835014417767525\n",
      "Hard dataset, Epoch: 7, Loss:  0.07519058883190155\n",
      "Hard dataset, Epoch: 7, Loss:  0.1354573518037796\n",
      "Hard dataset, Epoch: 7, Loss:  0.29405489563941956\n",
      "Hard dataset, Epoch: 7, Loss:  0.1391642689704895\n",
      "Hard dataset, Epoch: 7, Loss:  0.1674560308456421\n",
      "Hard dataset, Epoch: 7, Loss:  0.050452012568712234\n",
      "Hard dataset, Epoch: 7, Loss:  0.02025442384183407\n"
     ]
    }
   ],
   "source": [
    "# Start training for hard dataset\n",
    "for epoch in range(EPOCHS):\n",
    "    train(\"Hard\", hard_model, hard_training_loader, hard_optimizer, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the trained models\n",
    "\n",
    "In case you want to save the already trained models, uncomment the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(easy_model, 'easy_model.pt')\n",
    "#torch.save(medium_model, 'medium_model.pt')\n",
    "#torch.save(hard_model, 'hard_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Model Validation**\n",
    "\n",
    "During the validation stage we pass the unseen data(Testing Dataset) to the model. This step determines how good the model performs on the unseen data.\n",
    "\n",
    "This unseen data are the testing_dataloaders we created way before from the validation data.\n",
    "\n",
    "The cell belows defines two functions: `validation()` and `get_metrics()`. \n",
    "\n",
    "* `validation()`: Takes a trained model and a testing data loader as input. It evaluates the model on the testing data and returns the predicted outputs and the true targets. It also prints examples of predictions along with the corresponding text data.\n",
    "\n",
    "* `get_metrics()`: Takes a difficulty level, a trained model, and a testing data loader as input. It calls the `validation()` function to get the predicted outputs and true targets. Then, it calculates various evaluation metrics such as:\n",
    "- Accuracy\n",
    "- Balanced Accuracy \n",
    "- Precision\n",
    "- Recall\n",
    "- F1 Score \n",
    "\n",
    "This metrics are calculated based on the predicted outputs and true targets. Finally, it prints the calculated metrics for the specified difficulty level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform validation on the model\n",
    "def validation(model, testing_loader):\n",
    "    model.eval()\n",
    "    fin_targets=[]\n",
    "    fin_outputs=[]\n",
    "    example_predictions = []\n",
    "    idx = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(testing_loader, 0):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.float)\n",
    "            text_data = data['texts']\n",
    "            \n",
    "            outputs = model(ids, mask)\n",
    "\n",
    "            predictions = torch.sigmoid(outputs).cpu().detach().numpy().tolist()\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "            if idx < 5:\n",
    "                example_predictions.append({\n",
    "                    'texts': text_data[0],\n",
    "                    'prediction': predictions[0],\n",
    "                    'true_label': fin_targets[len(fin_targets) - len(targets)]\n",
    "                })\n",
    "\n",
    "                idx+=1\n",
    "\n",
    "    # Show examples of predictions along with the text after validation\n",
    "    for i, example in enumerate(example_predictions):\n",
    "        paragraphs = example['texts'].split('[CLS]')\n",
    "        \n",
    "        print(f\"Example {i + 1}:\\n\")\n",
    "        print(f\"Paragraph 1: {paragraphs[0]}\")\n",
    "        print(f\"Paragraph 2: {paragraphs[1]}\\n\")\n",
    "        print(f\"Prediction: {example['prediction']}, Ground truth: {example['true_label']}\\n\")\n",
    "                \n",
    "    return fin_outputs, fin_targets\n",
    "\n",
    "# Function to calculate metrics for the model\n",
    "def get_metrics(difficulty, model, testing_loader):\n",
    "\n",
    "    print(f\"Metrics for {difficulty} dataset:\")\n",
    "\n",
    "    outputs, targets = validation(model, testing_loader)\n",
    "    outputs = np.array(outputs) >= 0.5\n",
    "    accuracy = metrics.accuracy_score(targets, outputs)\n",
    "    balanced_accuracy = metrics.balanced_accuracy_score(targets, outputs)\n",
    "    precision = metrics.precision_score(targets, outputs)\n",
    "    recall = metrics.recall_score(targets, outputs)\n",
    "    f1 = metrics.f1_score(targets, outputs)\n",
    "    \n",
    "    print(f\"\\nAccuracy Score = {accuracy}\")\n",
    "    print(f\"Balanced accuracy Score = {balanced_accuracy}\")\n",
    "    print(f\"Precision Score = {precision}\")\n",
    "    print(f\"Recall Score = {recall}\")\n",
    "    print(f\"F1 Score = {f1}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating metrics for each difficulty\n",
    "\n",
    "We generate the metrics of each model using the validation data of its corresponding difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Easy dataset:\n",
      "Example 1:\n",
      "\n",
      "Paragraph 1: Faith was meant to be a healthy and disciplined choice one could make to consciously build their own belief system in their own journey to contentment, peace and happiness with life, but now it has been perverted to create small armies of people who have allowed their subconscious minds to be infiltrated with bigotry, greed, and judgement, which can never help them on their journey to happiness and inner peace, but provides them with a sense of belonging, unity, tribalism, superiority, and purpose nonetheless, and is unfortunately accompanied by the same mechanism that makes traditional Faith so effective by guarding the subconscious from accepting any new information that is not aligned with these trusted belief systems.\n",
      "Paragraph 2: He’s running low. A lot of attorneys won’t represent him now (there’s a joke in the legal world that says MAGA stands for “make attorneys get attorneys”). And I suspect those who are open to representing him do so because the RNC is currently covering his legal fees. If that goes away I suspect he’ll have some problems.\n",
      "\n",
      "Prediction: [0.9977572560310364], Ground truth: [1.0]\n",
      "\n",
      "Example 2:\n",
      "\n",
      "Paragraph 1: Honestly. If anyone in the world thinks the US government values spending money on its people and infrastructure over the military, I have a war to sell you.\n",
      "Paragraph 2: Exactly. If Rulepublican voters truly care about helping Ukraine, they'll vote for Democrats to ensure we keep supporting the war effort. They can say they're pro-Ukraine all they like, but actions speak louder than words. Who will they chose to be on Tuesday?\n",
      "\n",
      "Prediction: [0.9998272061347961], Ground truth: [1.0]\n",
      "\n",
      "Example 3:\n",
      "\n",
      "Paragraph 1: While Virtual Justice Clock is said to be an initiative to exhibit vital statistics of the justice delivery system at the court-level, giving the details of the cases instituted, cases disposed and pendency of cases; JustIS Mobile App 2.0 is a tool available to judicial officers for effective court and case management by monitoring pendency and disposal of not only his court but also for individual judges working under them.\n",
      "Paragraph 2: I wanted to ask something regarding this. Pakistani stance before 2019 was that \"India has illegal occupation on Kashmir and is not giving rights to kashmiris\".\n",
      "\n",
      "Prediction: [0.9990994930267334], Ground truth: [1.0]\n",
      "\n",
      "Example 4:\n",
      "\n",
      "Paragraph 1: In general, be courteous to others. Debate/discuss/argue the merits of ideas, don't attack people. Personal insults, shill or troll accusations, hate speech, any suggestion or support of harm, violence, or death, and other rule violations can result in a permanent ban.\n",
      "Paragraph 2: Yep. Growing up, I remember there being a distinct separation between Denver, Castle Rock, and the Springs. Last time I was out there, it was one, unbroken suburb between all of it. It was shocking.\n",
      "\n",
      "Prediction: [0.9998358488082886], Ground truth: [1.0]\n",
      "\n",
      "Example 5:\n",
      "\n",
      "Paragraph 1: In general, be courteous to others. Debate/discuss/argue the merits of ideas, don't attack people. Personal insults, shill or troll accusations, hate speech, any suggestion or support of harm, violence, or death, and other rule violations can result in a permanent ban.\n",
      "Paragraph 2: Thank you for this. Reminds us that these corporate pigs own these political stooges 99 times out of a 100.\n",
      "\n",
      "Prediction: [0.9998356103897095], Ground truth: [1.0]\n",
      "\n",
      "\n",
      "Accuracy Score = 0.9547223204810753\n",
      "Balanced accuracy Score = 0.8740036810480161\n",
      "Precision Score = 0.9644\n",
      "Recall Score = 0.9840816326530613\n",
      "F1 Score = 0.9741414141414142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(\"Easy\", easy_model, easy_testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Medium dataset:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sonic\\Desktop\\a\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 1:\n",
      "\n",
      "Paragraph 1: The value of used clothing is quite small. She'd have to use ebay, craigslist, and goodwill to prove how much damage was caused. She'd have to find the closest used item for value. It would likely be misdemeanor territory, and I don't know if cops would even bother charging a case like that, especially if they know you aren't going to talk to them. I will say if you talk to them and they can get you to admit any illegal actions (such as the illegal eviction) then they will 100% charge you because you admitted it. So keep your mouth shut. Block the ex and have no contact of any kind. If anyone knocks on your door ignore them, whether it's your ex or cops.\n",
      "Paragraph 2: You don't talk. If the police have enough to arrest you, they won't need to talk, they'll arrest you. If they are not yet arresting you, it's because they need you to say something--anything--that gives them just enough of a crack of light to make an arrest. So yes, you don't talk, and if arrested, plead not guilty, continue to not talk other than that, and ask for a court appointed attorney.\n",
      "\n",
      "Prediction: [0.6393948197364807], Ground truth: [1.0]\n",
      "\n",
      "Example 2:\n",
      "\n",
      "Paragraph 1: The answer is yes. You can get a DUI sitting in your driveway. For the purposes of DUI law, private roads that are accessible directly from the public road are also unlawful to drive on while intoxicated, or to be in physical control of a vehicle.\n",
      "Paragraph 2: Your question of \"is it illegal to hotbox on private property\" includes the possibility that you will be intoxicated by marijuana (indeed, that is your goal) while being in actual physical control of the vehicle, leading to a DUI arrest.\n",
      "\n",
      "Prediction: [0.0014361016219481826], Ground truth: [0.0]\n",
      "\n",
      "Example 3:\n",
      "\n",
      "Paragraph 1: Now, personally I think this is all bullshit. Prior to the last couple elections this idea of such widespread conspiracy and corruption swaying the vote is a novel concept and unprecedented. We’ve gone almost 250 years, election results used to be handwritten and delivered on horseback but now all of a sudden in the age of cameras and fingerprint scanners someone found a way to cheat the election.\n",
      "Paragraph 2: They had 60 chances in court to show one single shred of evidence of voter fraud in 2020 and whiffed on all of them. All they got was Trump’s entire legal team disbarred and disgraced. The intensive, months-long audits they themselves demanded, using audit firms they picked found more votes for Biden multiple times, particularly in Arizona and Wisconsin. No one outside of the craziest Q bubble seriously thinks the dems rigged 2020. Especially since they apparently neglected to rig themselves big majorities in Congress, state legislatures, governor’s races, etc, while they were busy screwing Trump.\n",
      "\n",
      "Prediction: [0.010111498646438122], Ground truth: [1.0]\n",
      "\n",
      "Example 4:\n",
      "\n",
      "Paragraph 1: ONly partially. They also had \"factories\" that produced finished goods within Florence and made a great deal from textiles and the sale of bulk goods.\n",
      "Paragraph 2: Next you have the banchi in mercato. This is what u/SweatCleansTheSuit is essentially talking about. INstitutionally they have their origins at the semi-regular \"fairs\" of Europe. They did not get profits from \"convenience fees\" per se, but by concealing profits in exchange rates. This is the institution where one could get a promissory note, but that didn't come until later in the Middle Ages. In the 11th century checks were unknown and transfer orders were given by word of mouth. These are transfer and deposit banks. These eventually became larger institution and more \"official\", but these ARE NOT the Medici bank.\n",
      "\n",
      "Prediction: [0.0013020873302593827], Ground truth: [0.0]\n",
      "\n",
      "Example 5:\n",
      "\n",
      "Paragraph 1: Whaley, Joachim. Germany and the Holy Roman Empire: Volume 1 (Oxford History of Early Modern Europe).\n",
      "Paragraph 2: Can we compare the economic strenh of those two groups immediately prior (or after) WWII to 1989 to get a better feel for whether that's the case? Have there been historic studies such as that?\n",
      "\n",
      "Prediction: [0.7939772605895996], Ground truth: [1.0]\n",
      "\n",
      "\n",
      "Accuracy Score = 0.7939126724505761\n",
      "Balanced accuracy Score = 0.7809053576604648\n",
      "Precision Score = 0.8043310131477185\n",
      "Recall Score = 0.6880582203109494\n",
      "F1 Score = 0.7416651809591728\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(\"Medium\", medium_model, medium_testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Hard dataset:\n",
      "Example 1:\n",
      "\n",
      "Paragraph 1: Is he even allowed to be around kids? I suggest reaching out to one of the counselors or case workers about your concerns. You could also reach out to a child abuse hotline to ask questions. But people you work with are mandatory reporters and have training to know what to do next.\n",
      "Paragraph 2: You should ask your supervisors what their plans are as you should be mandatory reporters, yes? That means even with HIPAA you have a duty to report the danger to authorities. However I don't know if this counts as immediate danger? But you shouldn't be able to get in trouble for erring on the side of caution with your report. I'm not sure if you need to inform the police or CPS or both, but there should be some mechanism for you with mandatory reporting.\n",
      "\n",
      "Prediction: [0.9952267408370972], Ground truth: [1.0]\n",
      "\n",
      "Example 2:\n",
      "\n",
      "Paragraph 1: On the plus side, now you probably have an easy out if you want to break your lease early! Otherwise, I think you already know your lease is NOT going to be renewed.\n",
      "Paragraph 2: As a landlord, if I do not put a clause in my lease about something I cannot then evict a tenant for said thing. Your landlord verbally told you to do something. She has to legally put it in writing and give you time to comply. Since it's not a violation of your lease she can't evict you but she does not have to renew your lease.\n",
      "\n",
      "Prediction: [0.9940550327301025], Ground truth: [1.0]\n",
      "\n",
      "Example 3:\n",
      "\n",
      "Paragraph 1: That’s actually a bad example. Most newer cars, and VWs of that vintage for sure, have special chips in the keys which can only be programmed by a dealer.\n",
      "Paragraph 2: Key codes are recorded by the manufacturer when the car is made, which is why dealers can produce them by doing a VIN lookup. I don't know of any general locksmith that can do this, unless they are somehow authorized by a dealership or manufacturer.\n",
      "\n",
      "Prediction: [0.997998058795929], Ground truth: [1.0]\n",
      "\n",
      "Example 4:\n",
      "\n",
      "Paragraph 1: If KCMO has an Office of the Inspector General (a quick Google doesn't turn one up) or Ethics Hotline this is exactly the type of thing that those are there to investigate.\n",
      "Paragraph 2: Anyone can request anything... whether the city responds to those requests is typically a matter of local policy (it's been a while since I've read the Manual on Uniform Traffic Control Devices [MUTCD] but I don't remember anything pertaining to parking specifically, aside from the forma of the signs) I'd start by calling the city department of Public Works ( who typically oversee these types of things and they can confirm if its a legitimate sign and if so any local processes to challenge the placement if legitimate. They're also most likely the ones to most expeditiously correct the situation if they were placed without authorization.\n",
      "\n",
      "Prediction: [0.001352666411548853], Ground truth: [0.0]\n",
      "\n",
      "Example 5:\n",
      "\n",
      "Paragraph 1: Castle doctrine is totally different. Castle doctrine basically says your home is your castle, and you can defend it with deadly force with no duty to retreat from an invader or use less lethal means of defense.\n",
      "Paragraph 2: In a castle-doctrine jurisdiction, generally you have a duty to retreat when outside of your home (and sometimes vehicle). If you use force in self-defence when you could retreat instead, you're breaking the law, unless you're inside your home at the time.\n",
      "\n",
      "Prediction: [0.001952816266566515], Ground truth: [1.0]\n",
      "\n",
      "\n",
      "Accuracy Score = 0.7705596107055961\n",
      "Balanced accuracy Score = 0.771534435345417\n",
      "Precision Score = 0.7426647426647427\n",
      "Recall Score = 0.7909836065573771\n",
      "F1 Score = 0.7660630116596379\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(\"Hard\", hard_model, hard_testing_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Model comparison**\n",
    "Here we will show the comparison of `RoBERTa` to other base transformer models to see why is it better. The models we will test are `BERT` and `DistilBERT`. For the sake of time, we will only test the easy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (15729, 2)\n",
      "TRAIN Dataset: (12902, 2)\n",
      "TEST Dataset: (2827, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bert_easy_training_loader, bert_easy_testing_loader = create_dataloader(easy_df_train, easy_df_validation, \n",
    "                                                                        BertTokenizer.from_pretrained('bert-base-cased'), easy_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (15729, 2)\n",
      "TRAIN Dataset: (12902, 2)\n",
      "TEST Dataset: (2827, 2)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distilbert_easy_training_loader, distilbert_easy_testing_loader = create_dataloader(easy_df_train, easy_df_validation, \n",
    "                                                                                    DistilBertTokenizer.from_pretrained('distilbert-base-cased'), easy_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RoBERTaClass\n",
    "class BERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.dropout = 0.5\n",
    "        self.hidden_embd = 768\n",
    "        self.output_layer = 1\n",
    "        # Declare the layers here\n",
    "        self.l1 = transformers.RobertaModel.from_pretrained('bert-base-cased')\n",
    "        self.l2 = torch.nn.Dropout(self.dropout)\n",
    "        self.l3 = torch.nn.Linear(self.hidden_embd, self.output_layer)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        # Use the transformer, then the dropout and the linear in that order.\n",
    "        _, output_1 = self.l1(ids, attention_mask = mask, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the RoBERTaClass\n",
    "class DistilBERTClass(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DistilBERTClass, self).__init__()\n",
    "        self.dropout = 0.5\n",
    "        self.hidden_embd = 768\n",
    "        self.output_layer = 1\n",
    "        # Declare the layers here\n",
    "        self.l1 = transformers.RobertaModel.from_pretrained('distilbert-base-cased')\n",
    "        self.l2 = torch.nn.Dropout(self.dropout)\n",
    "        self.l3 = torch.nn.Linear(self.hidden_embd, self.output_layer)\n",
    "\n",
    "    def forward(self, ids, mask):\n",
    "        # Use the transformer, then the dropout and the linear in that order.\n",
    "        _, output_1 = self.l1(ids, attention_mask = mask, return_dict=False)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['encoder.layer.8.output.dense.bias', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'pooler.dense.bias', 'embeddings.position_embeddings.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.attention.self.value.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.output.LayerNorm.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'pooler.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.0.output.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the RoBERTaClass for easy dataset\n",
    "bert_easy_model = BERTClass().to(device)\n",
    "\n",
    "bert_easy_optimizer = torch.optim.Adam(params = bert_easy_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.10.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.8.attention.output.dense.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.1.attention.self.query.weight', 'pooler.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.3.attention.self.key.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.intermediate.dense.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.7.attention.self.key.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.8.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'pooler.dense.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.6.output.dense.weight', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.11.output.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Create instances of the RoBERTaClass for easy dataset\n",
    "distilbert_easy_model = DistilBERTClass().to(device)\n",
    "\n",
    "distilbert_easy_optimizer = torch.optim.Adam(params = distilbert_easy_model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\sonic\\Desktop\\a\\env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT Easy dataset, Epoch: 0, Loss:  0.7603780031204224\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.3703303337097168\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.11205479502677917\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.36880260705947876\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.18664789199829102\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.23761683702468872\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.4722900390625\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.5425524711608887\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.3263493776321411\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.41518545150756836\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.5762661695480347\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.38620203733444214\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.5239861607551575\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.3394910395145416\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.31254613399505615\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.6607599258422852\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.33252453804016113\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.5230900049209595\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.43782877922058105\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.37856292724609375\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.45642024278640747\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.21945999562740326\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.3905692398548126\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.37496018409729004\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.20707039535045624\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.5415781736373901\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.14958611130714417\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.4536188542842865\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.2954027056694031\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.16762718558311462\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.39449813961982727\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.2782067060470581\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.20086202025413513\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.33688056468963623\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.4369294047355652\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.6792968511581421\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.4887739419937134\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.19402214884757996\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.4480523467063904\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.3938087224960327\n",
      "BERT Easy dataset, Epoch: 0, Loss:  0.33701714873313904\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.32057926058769226\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.42054352164268494\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.41777682304382324\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.42941421270370483\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.24839049577713013\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.2980389893054962\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.4678936004638672\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.2594601809978485\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.3321648836135864\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.18269093334674835\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.5619795322418213\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.36677658557891846\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.5118265151977539\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.2508823275566101\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.5511012077331543\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.23869585990905762\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.4516128599643707\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.28091877698898315\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.4493997097015381\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.35211843252182007\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.37253329157829285\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.22778767347335815\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.4183368682861328\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.30210036039352417\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.3975655138492584\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.5376859903335571\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.21568475663661957\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.35407257080078125\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.14691105484962463\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.3035266697406769\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.3504542112350464\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.3127557039260864\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.16226571798324585\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.3597975969314575\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.3413239121437073\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.18688508868217468\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.5242873430252075\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.22443275153636932\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.49550163745880127\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.20932281017303467\n",
      "BERT Easy dataset, Epoch: 1, Loss:  0.19645428657531738\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.1789783388376236\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.15900184214115143\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.5015413165092468\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.1829337775707245\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.291858434677124\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.08341369032859802\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.3337548077106476\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.34475764632225037\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.4501771926879883\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.18320046365261078\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.2327662855386734\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.193618044257164\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.22955086827278137\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.20943298935890198\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.36379480361938477\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.12880900502204895\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.1466425359249115\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.34470751881599426\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.1554470807313919\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.2938148081302643\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.40405064821243286\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.27106788754463196\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.3400849997997284\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.19744989275932312\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.178564190864563\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.501911461353302\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.3210902810096741\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.3552435636520386\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.20028910040855408\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.3521287143230438\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.29941126704216003\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.1180996373295784\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.15867416560649872\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.31488728523254395\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.31511741876602173\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.30766987800598145\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.07460430264472961\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.2085268348455429\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.1494651734828949\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.2871033251285553\n",
      "BERT Easy dataset, Epoch: 2, Loss:  0.3577181100845337\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.07005665451288223\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.23619204759597778\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.23291413486003876\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.10801170021295547\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.1584385186433792\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.2214505970478058\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.13088592886924744\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.3582378625869751\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.24736303091049194\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.12344801425933838\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.20094411075115204\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.14241379499435425\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.06858263909816742\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.18831396102905273\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.1177462786436081\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.14211076498031616\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.2694298326969147\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.10219016671180725\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.03714820742607117\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.13382841646671295\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.030379079282283783\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.19706103205680847\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.24905505776405334\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.38248565793037415\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.3410710096359253\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.15000052750110626\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.02836478129029274\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.09462825953960419\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.24551308155059814\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.21510876715183258\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.10981341451406479\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.07710055261850357\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.05593607574701309\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.02616017684340477\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.15020206570625305\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.026840779930353165\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.23087742924690247\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.17057491838932037\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.30159085988998413\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.0378250926733017\n",
      "BERT Easy dataset, Epoch: 3, Loss:  0.18017368018627167\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.2048988789319992\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.2620857357978821\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.2789050042629242\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.15785038471221924\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.10094189643859863\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.2566264271736145\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.19159021973609924\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.10910811275243759\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.2291950285434723\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.13170374929904938\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.22476692497730255\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.17532740533351898\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.15173737704753876\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.09927847981452942\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.2013424187898636\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.16942936182022095\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.23326393961906433\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.21723003685474396\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.1891232132911682\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.24245676398277283\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.2378496527671814\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.1335587352514267\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.10204292833805084\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.27015089988708496\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.1678576022386551\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.16117286682128906\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.35820508003234863\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.27850744128227234\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.12836456298828125\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.07063549011945724\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.2837275564670563\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.13211482763290405\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.12101288884878159\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.09180094301700592\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.12863750755786896\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.04061799496412277\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.20698484778404236\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.22849152982234955\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.08805248141288757\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.32490336894989014\n",
      "BERT Easy dataset, Epoch: 4, Loss:  0.18956376612186432\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.1465519219636917\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.36024898290634155\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.2986658215522766\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.26872938871383667\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.31130799651145935\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.18562664091587067\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.15895378589630127\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.3540395498275757\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.3182923495769501\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.1578378677368164\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.10244657099246979\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.06848005205392838\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.24094435572624207\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.18075712025165558\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.07703480124473572\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.05541189759969711\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.043067675083875656\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.17097152769565582\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.11211583763360977\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.09063117206096649\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.08717097342014313\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.030958041548728943\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.06078696250915527\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.27795886993408203\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.26700589060783386\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.14200475811958313\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.059283096343278885\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.16902242600917816\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.24914824962615967\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.2725253105163574\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.1082623079419136\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.3038133382797241\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.15901584923267365\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.05682922899723053\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.04628009349107742\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.020639250054955482\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.013125399127602577\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.20327338576316833\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.36007967591285706\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.03674141317605972\n",
      "BERT Easy dataset, Epoch: 5, Loss:  0.16311518847942352\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.12326710671186447\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.25166481733322144\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.14006493985652924\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.06938670575618744\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.17772747576236725\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.13868696987628937\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.10639236122369766\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.14486318826675415\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.14485208690166473\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.05559336021542549\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.09832049161195755\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.3052430748939514\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.07794361561536789\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.06650002300739288\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.047145769000053406\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.08600792288780212\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.03502745181322098\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.03841135650873184\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.13241314888000488\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.15151512622833252\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.054531075060367584\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.06517713516950607\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.03155722841620445\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.08888785541057587\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.029414519667625427\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.052636485546827316\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.01764175295829773\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.12818317115306854\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.05443238466978073\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.022793659940361977\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.05864373594522476\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.22743982076644897\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.12790444493293762\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.11372300982475281\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.27579009532928467\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.33310234546661377\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.13194620609283447\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.043889373540878296\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.24362972378730774\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.1005946546792984\n",
      "BERT Easy dataset, Epoch: 6, Loss:  0.05463927239179611\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.0712025910615921\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.22464455664157867\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.06189805269241333\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.04699833691120148\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.3039920926094055\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.054175518453121185\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.0394267737865448\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.010638386011123657\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.1563345193862915\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.06539569050073624\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.16802561283111572\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.028632571920752525\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.02828199416399002\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.024770483374595642\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.1744431108236313\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.1590130627155304\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.05957353860139847\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.22235289216041565\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.02531326562166214\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.05266889929771423\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.1836770623922348\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.08785998821258545\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.012132687494158745\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.03583575412631035\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.14314711093902588\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.07674548029899597\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.11840614676475525\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.13237537443637848\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.2114534080028534\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.08815349638462067\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.057797521352767944\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.013494236394762993\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.052412670105695724\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.0712856650352478\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.07257895916700363\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.013854920864105225\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.07945466041564941\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.2470167875289917\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.17423243820667267\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.1522284299135208\n",
      "BERT Easy dataset, Epoch: 7, Loss:  0.14405591785907745\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(\"BERT Easy\", bert_easy_model, bert_easy_training_loader, bert_easy_optimizer, epoch)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for BERT Easy dataset:\n",
      "Example 1:\n",
      "\n",
      "Paragraph 1: EIC ships were turned away from ports in the rest of the colonies before they could offload their cargo, but the Governor of Massachusetts, Thomas Hutchinson, refused to give in to popular pressure and refused to allow the British ships to return to England (at issue here was the payment of import duties to the colonies, different than the waived export duties owed to England - Hutchinson's sons and some of his friends were consignees of the tea shipment, and not only would the colony lose the import duty payment, but also the potential income from the sale of the tea. He had a lot to lose, personally, in both real and political terms). When several ultimatums had come and gone, a general meeting of citizens at the Old South Meeting House broke up with \"a general huzza for Griffin's wharf.\".\n",
      "Paragraph 2: In addition to the excellent reply above, it's important to note that although the so-called \"Boston Tea Party\" was a relatively local event, and therefore could have been organized by New England-based smugglers, it was only one occurrence in a broader protest against the tax on tea that could never have been pulled off by smugglers. When EIC tea arrived in other ports, including Philadelphia, New York, and Charleston, locals also protested and largely stopped the tea from being imported. For example, discontent had stoked the colonists of Charles Town, SC into a frenzy on December 1, 1773, weeks before the events in Boston. In Charles Town, the colonists were aware that of laws concerning unloaded cargo in the harbor and refused to allow the London to disembark more than 200 chests of EIC tea. With the tea essentially forfeited by the ship's investors, it was sized by custom agents for nonpayment of the tax and moved to the Exchange where it was locked away. While this may have been a short-term boon for local smugglers, to suggest they whipped up the entire town is unfathomable. As smugglers were largely independent workers, it is incredibly unlikely to believe they orchestrated such a large protest. Furthermore, it is impossible to think smugglers in Charlestown relayed their message roughly one thousand miles north.\n",
      "\n",
      "Prediction: [0.9232154488563538], Ground truth: [1.0]\n",
      "\n",
      "Example 2:\n",
      "\n",
      "Paragraph 1: Consider first that that's not the worst thing Dante Alighieri writes about the Church as a contemporary institution and about the actions of individual Popes. In Inferno XIX, for instance, Dante bumps into Pope Nicholas III (1277-1280) among the simoniacs, i.e., those who sell Church offices and assets for personal gain. Nicholas and the Simoniacs (a potentially great name for a band, incidentally) are half buried upside down and their feet are set on fire. Because Nicholas cannot see Dante, he mistakes him for Boniface VIII (1294-1303) and even predicts that he will soon be joined among the Simoniacs by Clement V (1305-1314). Indeed the historical Dante Alighieri had direct beef with Boniface VIII, who is said to have operated for the poet's exile from Florence in 1302 (still several years before he started composing the Commedia).\n",
      "Paragraph 2: Oh ok. I thought those were generally about his life. I'd like to read something that analyzes the whole Comedy or even just a particular book. I haven't read it yet. But I'd like something to read soon after. I find criticism about the Catholic Church in those times very interesting.\n",
      "\n",
      "Prediction: [0.9866812229156494], Ground truth: [1.0]\n",
      "\n",
      "Example 3:\n",
      "\n",
      "Paragraph 1: You're manipulating. The article clearly states the reason why payments stopped was because the Brazilian government breached the contract agreement by restarting deforestation.\n",
      "Paragraph 2: Yup. Plus regrowth is slow. If you cut down and burn a 100 year-old forest, it will take about 100 years to absorb back the CO2 that was released. In the best-case scenario.\n",
      "\n",
      "Prediction: [0.9605700373649597], Ground truth: [1.0]\n",
      "\n",
      "Example 4:\n",
      "\n",
      "Paragraph 1: I must point out however, that if the purpouse of the recommendation was to argue that spaniards werent in charge, it had the opposite effect in me, rather enforcing much of the conceptions and its role as a polity that was thouroughly subservient to the spaniards, and confirming many of my ideas about Tlaxcalan society of the time.\n",
      "Paragraph 2: I found no evidence of that here, if anything, I personally found that the article unintentionally argued the opposite, that the Tlaxcalans didnt have such a thing beyond the isolated actions of some individuals (unlike MANY other native groups that actively resisted spaniard interference even in the Valley of Mexico itself were it was the most difficult), in this regard I found your initial arguments on the subject much more interesting rather than this last one, for that reason I think I will it keep separate on the back of my mind and spin it for a while, who knows? maybe in a few weeks I'll get a new perspective on it, again, thanks for the recommendation, I found it very interesting and complete to read, I've definitely saved it for future reference.\n",
      "\n",
      "Prediction: [0.9314857721328735], Ground truth: [0.0]\n",
      "\n",
      "Example 5:\n",
      "\n",
      "Paragraph 1: I'm actually not exactly sure what you were trying to say , because you seemed to criticize a book called \"Romance of the Three Kingdoms\" for over-romantizing the era, and dislike mass media productions for over-glorifying.\n",
      "Paragraph 2: You talk of genocide. Not unknown, Duan Jiong's campaigns against the Qiang in the late 160s have been called that, but it isn't a term used about the three kingdoms, even involving some of the most brutal accusations (Cao Cao in Xu, Sima Yi after conquering the Gongsun clan) I haven't seen professional historians use the term. Nor does anything compare to the Holocaust that I can think of.\n",
      "\n",
      "Prediction: [0.8413313031196594], Ground truth: [1.0]\n",
      "\n",
      "\n",
      "Accuracy Score = 0.8475415634948709\n",
      "Balanced accuracy Score = 0.5585546473231202\n",
      "Precision Score = 0.8810872027180068\n",
      "Recall Score = 0.9526530612244898\n",
      "F1 Score = 0.9154736222788783\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "get_metrics(\"BERT Easy\", bert_easy_model, bert_easy_testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.8001834154129028\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.4950113594532013\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.4724515974521637\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.40167802572250366\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.552944540977478\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.3800603747367859\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.6455175280570984\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.47416335344314575\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.2460191398859024\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.256367564201355\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.32808569073677063\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.5097025036811829\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.41308775544166565\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.35733169317245483\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.2287450134754181\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.5161739587783813\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.3091129958629608\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.24221616983413696\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.25978782773017883\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.3322894871234894\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.22315770387649536\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.5860874652862549\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.18884456157684326\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.3140867352485657\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.4077408015727997\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.31893396377563477\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.4783511161804199\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.23658117651939392\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.29406455159187317\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.31072384119033813\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.7021039128303528\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.2664204239845276\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.2567196190357208\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.38129234313964844\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.3010607957839966\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.39631548523902893\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.4790767431259155\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.40031781792640686\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.44084227085113525\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.4502423107624054\n",
      "DistilBERT Easy dataset, Epoch: 0, Loss:  0.30157965421676636\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.3309498429298401\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.371581494808197\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.19220709800720215\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.1671544909477234\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.5891866087913513\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.22886258363723755\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.4603341817855835\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.3716337978839874\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.21266698837280273\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.4923308491706848\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.2402603030204773\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.3081532120704651\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.39207008481025696\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.5413576364517212\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.4332408308982849\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.24463078379631042\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.25883758068084717\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.3333093523979187\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.34118807315826416\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.41923949122428894\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.2373449057340622\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.2622082531452179\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.26174864172935486\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.24866047501564026\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.32374465465545654\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.4155797064304352\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.14527592062950134\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.3912031650543213\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.3990153670310974\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.31647011637687683\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.2592196464538574\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.34528836607933044\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.3845120370388031\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.4604988098144531\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.3037756383419037\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.3656889796257019\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.1801055669784546\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.3233572840690613\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.3921005427837372\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.37819144129753113\n",
      "DistilBERT Easy dataset, Epoch: 1, Loss:  0.2155541479587555\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.324836403131485\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.2809563875198364\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.17081162333488464\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.260903000831604\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.14711275696754456\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.07694694399833679\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.2687682807445526\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.3578270673751831\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.18720701336860657\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.23596255481243134\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.17511777579784393\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.2929561734199524\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.32046622037887573\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.17594441771507263\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.14299236238002777\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.25327906012535095\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.47041642665863037\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.18716365098953247\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.151056170463562\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.10737937688827515\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.31213659048080444\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.2387954294681549\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.30886226892471313\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.5689599514007568\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.31441110372543335\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.20565015077590942\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.1452747881412506\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.0777340680360794\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.28268682956695557\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.5376113653182983\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.34486615657806396\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.1675778180360794\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.17280042171478271\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.11934087425470352\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.09058926999568939\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.2910100519657135\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.2720608711242676\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.2603849172592163\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.24838121235370636\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.20558851957321167\n",
      "DistilBERT Easy dataset, Epoch: 2, Loss:  0.4869956374168396\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.2680244743824005\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.13182823359966278\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.23530124127864838\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.1631373167037964\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.1541922241449356\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.014584037475287914\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.23795901238918304\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.12612982094287872\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.22147120535373688\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.019875869154930115\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.08987326920032501\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.18866407871246338\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.320583701133728\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.2920994758605957\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.12347856163978577\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.3191468119621277\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.10651165246963501\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.30672362446784973\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.03909444063901901\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.25789934396743774\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.18741339445114136\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.47379547357559204\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.09965158253908157\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.27178770303726196\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.11736787855625153\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.19611914455890656\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.33159422874450684\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.14766617119312286\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.20948415994644165\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.16145221889019012\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.2023964524269104\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.2549765706062317\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.22312937676906586\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.0554903969168663\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.11552155762910843\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.10323337465524673\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.359843909740448\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.16652321815490723\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.12371259927749634\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.11107736825942993\n",
      "DistilBERT Easy dataset, Epoch: 3, Loss:  0.0924379825592041\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.20360803604125977\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.17438095808029175\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.1694003939628601\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.042851075530052185\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.1779983937740326\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.0440620481967926\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.08096693456172943\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.04426963999867439\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.07091253995895386\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.14341259002685547\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.19677269458770752\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.28023067116737366\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.041478920727968216\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.07286214083433151\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.38549575209617615\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.12711979448795319\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.27529335021972656\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.1048261746764183\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.13371962308883667\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.21302327513694763\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.0669863149523735\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.1369803547859192\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.2457008957862854\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.20890817046165466\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.09072942286729813\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.22397908568382263\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.10447394847869873\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.24409128725528717\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.08095806837081909\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.19000911712646484\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.07597646117210388\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.04346441477537155\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.2588859796524048\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.08160103857517242\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.13426105678081512\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.09098105877637863\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.05197774991393089\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.18860207498073578\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.10997480154037476\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.10886605083942413\n",
      "DistilBERT Easy dataset, Epoch: 4, Loss:  0.23723049461841583\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.13585835695266724\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.1051819771528244\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.17997640371322632\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.24700400233268738\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.029784630984067917\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.023347700014710426\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.014598291367292404\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.15488852560520172\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.31029051542282104\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.2829360365867615\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.23895767331123352\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.22421897947788239\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.3218361735343933\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.11710889637470245\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.21134839951992035\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.03407714515924454\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.16820524632930756\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.08342074602842331\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.07424917817115784\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.10254909098148346\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.06299964338541031\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.05053189396858215\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.12869885563850403\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.08421286940574646\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.23822195827960968\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.09096284210681915\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.28084975481033325\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.0366072878241539\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.09475920349359512\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.29278573393821716\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.03350253403186798\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.005649724509567022\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.22445523738861084\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.008147056214511395\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.12311301380395889\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.1870233118534088\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.23259702324867249\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.08065807819366455\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.33994340896606445\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.06175145134329796\n",
      "DistilBERT Easy dataset, Epoch: 5, Loss:  0.05564244091510773\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.20689105987548828\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.04654379189014435\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.38868701457977295\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.16088709235191345\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.21539245545864105\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.02873198874294758\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.03716804087162018\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.09794916957616806\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.025172941386699677\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.04743552207946777\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.03819437325000763\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.093641497194767\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.28174248337745667\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.09298324584960938\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.16119679808616638\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.1466398537158966\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.27997133135795593\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.3759327530860901\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.028595831245183945\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.01572238653898239\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.15482981503009796\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.0220959410071373\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.04926784336566925\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.04039400443434715\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.09882768243551254\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.14104920625686646\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.3131178617477417\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.2959390878677368\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.15115602314472198\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.0867731124162674\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.16153757274150848\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.2263173907995224\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.1685132086277008\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.057702504098415375\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.3199619650840759\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.1293187290430069\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.018564345315098763\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.21396812796592712\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.014743850566446781\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.23832577466964722\n",
      "DistilBERT Easy dataset, Epoch: 6, Loss:  0.13201148808002472\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.1059677004814148\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.0458139143884182\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.042921263724565506\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.30244582891464233\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.025743111968040466\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.3615301251411438\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.06573082506656647\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.07693959772586823\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.06044609844684601\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.0435030497610569\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.06192655861377716\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.07975174486637115\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.023408612236380577\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.3423267602920532\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.011540264822542667\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.2919026017189026\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.2195982187986374\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.07513400912284851\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.06455038487911224\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.07681877166032791\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.01835760474205017\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.08514704555273056\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.024522744119167328\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.08660329133272171\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.06163930147886276\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.014556173235177994\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.017293788492679596\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.15116964280605316\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.15471771359443665\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.045434609055519104\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.19423770904541016\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.10138068348169327\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.16051940619945526\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.10822273790836334\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.0494820699095726\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.18762712180614471\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.02837538905441761\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.04438123479485512\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.32182109355926514\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.13814491033554077\n",
      "DistilBERT Easy dataset, Epoch: 7, Loss:  0.1509150266647339\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "    train(\"DistilBERT Easy\", distilbert_easy_model, distilbert_easy_training_loader, distilbert_easy_optimizer, epoch)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for DistilBERT Easy dataset:\n",
      "Example 1:\n",
      "\n",
      "Paragraph 1: Others had expressed openings to ideas of a collaboration with other nationalist groups (Mazzini after all had promoted the ideal of a community of the nations, which were to be regarded as “the individuals of humankind”, the nations being to humankind as citizens were to the nation), or rather to favor the idea of a cooperation of the nationalities (notably, the Austro-Hungarian Empire was sitting atop the many nationalities of the Balkans, even if the relations between the Italian Nationalists and the Triple Alliance are rather complicated and ambivalent); but those inclinations – which played into the traditional “democratic” tendencies towards an expansion which was mindful of the rights of the other nationalities, yet opposed to the natural expansive direction of the Italian imperialism, which looked for an affirmation within the Mediterranean – were among the first to be restrained and quelled. One of the rising stars of Italian Nationalism (and one of its more outspoken polemists), the publicist Luigi Federzoni explained that “the Balkan Slavs were the kind of people […] the hypothetical suppression of which from the geographical map of the world would not have changed one iota in the history of civilization”. The Mediterranean and Adriatic policies of the Nationalists would remain a point of contention in the subsequent years, given the relevance of the matter (perhaps even more polemical than political) in the years of the Great War.\n",
      "Paragraph 2: The Nationalists were at risk – pointed out Olindo Malagodi in April 1912, during a long polemics between the Nationalist “direction” (Corradini, Federzoni, Maraviglia, etc.) and the “democratic” fraction of the Association, inspired by the publication of a letter from Charles Maurras, of distinctive antisemitic tone – of ending up like their French counterparts, a movement of a reactionary minority, “xenophobic and antisemitic”. At the same time, the Libyan War had revealed a new willingness of the catholic world to participate to the “national” life – which perhaps called for a reconsideration of the entire political collocation of the Nationalists in relation to the moderate catholics.\n",
      "\n",
      "Prediction: [0.927715539932251], Ground truth: [0.0]\n",
      "\n",
      "Example 2:\n",
      "\n",
      "Paragraph 1: was still ripe for attacks, and of course winter was setting in, and hunger was even worse, so there was additional support from General Frost. Even the Russians were suffering greatly.\n",
      "Paragraph 2: With regards to the original question it might also be worth mentioning the diplomatic kerfuffle in 2004 when the 100th anniversary of the Entente Cordiale was held in the Waterloo Chamber at Windsor Castle, which necessitated a temporary name change and the covering up of the paintings in the room.\n",
      "\n",
      "Prediction: [0.8074591159820557], Ground truth: [1.0]\n",
      "\n",
      "Example 3:\n",
      "\n",
      "Paragraph 1: This words are key in understanding his Seventh. A grand, majestic piece, filled with sorrowful longing and melodies that evoque that which he lacked, companionship, love, heartfelt encounters. But with a distinctive movement that defies those feelings, with a motif that repeats itself in the final movement: the Allegretto, the second movement, the most well known of them all, often performed separately. The motif is clear, that \"kingdom in the air\" is written in the score: his isolation is marked by the oppressive rhythm, which shrouds the melody. But, according to Sir George Grove in his Beethoven and his Nine Symphonies (1896) the melody is set free in the recurring motif of what he calles \"the temple\"; a harmonious melody carried by violins and repeated by winds, signifying a refuge, a shelter that protects the purity of the joy, that will be a key element of the final movement, Allegro con brio, filled with dances and ritornellos (immediately repeated chords or even entire melodies).\n",
      "Paragraph 2: Marzona, A. - Les incidents franco-italiens de Fiume ou l’expression des frustrations italiennes (novembre1918-juillet1919), Revue historique des armées, 254, 2009, 29-38.\n",
      "\n",
      "Prediction: [0.9851102828979492], Ground truth: [1.0]\n",
      "\n",
      "Example 4:\n",
      "\n",
      "Paragraph 1: Second, when China intervened and MacArthur proposed using nuclear weapons or bombing Chinese cities, Truman sacked him, and reiterated that the US was not waging a war of conquest - it was enforcing a UN Security Council Resolution.\n",
      "Paragraph 2: First, Truman only permitted General MacArthur to advance across the 38th Parallel in 1950 because he was assured that China and the USSR would not intervene and victory was imminent.\n",
      "\n",
      "Prediction: [0.9711059927940369], Ground truth: [0.0]\n",
      "\n",
      "Example 5:\n",
      "\n",
      "Paragraph 1: Ethics violations aren’t necessarily legal violations. If they are concerned about the laws being broken why not refer to the proper legal body? Ethics issues I think can be largely centered around very bad even immoral behavior that isn’t criminal but needs addressing. I think they know this.\n",
      "Paragraph 2: For those who have questions regarding any media outlets being posted on this subreddit, please click to review our details as to our approved domains list and outlet criteria.\n",
      "\n",
      "Prediction: [0.9960989952087402], Ground truth: [1.0]\n",
      "\n",
      "\n",
      "Accuracy Score = 0.8510788822072869\n",
      "Balanced accuracy Score = 0.6021160612786228\n",
      "Precision Score = 0.8924564796905222\n",
      "Recall Score = 0.9416326530612245\n",
      "F1 Score = 0.9163853028798411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "get_metrics(\"DistilBERT Easy\", distilbert_easy_model, distilbert_easy_testing_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RoBERTa`:\n",
    "\n",
    "- Accuracy Score = 0.9547\n",
    "- Balanced Accuracy Score = 0.8740\n",
    "- Precision Score = 0.9644\n",
    "- Recall Score = 0.9841\n",
    "- F1 Score = 0.9741\n",
    "\n",
    "`BERT`:\n",
    "\n",
    "- Accuracy Score = 0.8475\n",
    "- Balanced Accuracy Score = 0.5586\n",
    "- Precision Score = 0.8811\n",
    "- Recall Score = 0.9527\n",
    "- F1 Score = 0.9155\n",
    "\n",
    "`DistilBERT`:\n",
    "\n",
    "- Accuracy Score = 0.8511\n",
    "- Balanced Accuracy Score = 0.6021\n",
    "- Precision Score = 0.8925\n",
    "- Recall Score = 0.9416\n",
    "- F1 Score = 0.9164\n",
    "\n",
    "\n",
    "By comparing the metrics obtanined of each model, we come to the conclusion that RoBERTa is the winner. In general, RoBERTa outperforms the other two models on all metrics, with higher accuracy, recall and F1-score, indicating a better ability to correctly classify positive and negative samples. It also has a very high precision and recall score, suggesting that it minimises both false positives and false negatives.\n",
    "\n",
    "BERT performs better than DistilBERT on most metrics, but still lags behind RoBERTa in terms of overall accuracy and ability to balance precision and recall.\n",
    "\n",
    "In conclusion, RoBERTa appears to be the superior model in this comparison, as it performs higher on all metrics evaluated."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
